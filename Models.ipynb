{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Different Machine Learning Models"
      ],
      "metadata": {
        "id": "ndi8Si3k6ToV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('t_dataset.csv', index_col=False)"
      ],
      "metadata": {
        "id": "soKKbFRi4X1M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Long Short-Term Memory (LSTM) Networks:\n",
        "\n"
      ],
      "metadata": {
        "id": "3BG9XCiy_Uo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "max_len = 100  # Maximum sequence length\n",
        "vocab_size = 10000  # Limit on the number of words in vocabulary\n",
        "embedding_dim = 128  # Dimensionality of word embeddings\n",
        "\n",
        "# Load preprocessed data\n",
        "texts = df['Tweet']  # List to store your text data (sarcastic and non-sarcastic)\n",
        "labels = df['Sarcasm']  # List to store labels (1 for sarcastic, 0 for non-sarcastic)\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Bidirectional LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # Use Bidirectional LSTM\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model (replace with desired evaluation metrics)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new data (optional)\n",
        "new_text = \"This new restaurant is a real gem, NOT.\"  # Replace with your text\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "if prediction > 0.5:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGrgtQoU9HxU",
        "outputId": "7bd88c6e-d77e-442c-a525-261eb234c8a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 28s 259ms/step - loss: 0.5872 - accuracy: 0.6686 - val_loss: 0.4304 - val_accuracy: 0.8124\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 19s 253ms/step - loss: 0.3646 - accuracy: 0.8414 - val_loss: 0.4784 - val_accuracy: 0.7705\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 18s 235ms/step - loss: 0.1834 - accuracy: 0.9341 - val_loss: 0.5519 - val_accuracy: 0.7554\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 17s 226ms/step - loss: 0.0947 - accuracy: 0.9706 - val_loss: 0.6741 - val_accuracy: 0.7504\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 19s 253ms/step - loss: 0.0567 - accuracy: 0.9828 - val_loss: 0.7980 - val_accuracy: 0.7303\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 17s 231ms/step - loss: 0.0274 - accuracy: 0.9937 - val_loss: 0.9307 - val_accuracy: 0.7605\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 17s 232ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 1.0631 - val_accuracy: 0.7320\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 18s 246ms/step - loss: 0.0135 - accuracy: 0.9971 - val_loss: 1.0858 - val_accuracy: 0.7454\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 18s 242ms/step - loss: 0.0137 - accuracy: 0.9962 - val_loss: 1.1182 - val_accuracy: 0.7370\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 17s 229ms/step - loss: 0.0104 - accuracy: 0.9979 - val_loss: 1.2059 - val_accuracy: 0.7521\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 1.2059 - accuracy: 0.7521\n",
            "Test Loss: 1.2059, Accuracy: 0.7521\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "This text is predicted to be non-sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Gated Recurrent Units (GRUs):"
      ],
      "metadata": {
        "id": "CeZ5LIhL_aD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "max_len = 100  # Maximum sequence length\n",
        "vocab_size = 10000  # Limit on the number of words in vocabulary\n",
        "embedding_dim = 128  # Dimensionality of word embeddings\n",
        "\n",
        "# Load preprocessed data (replace with your actual data)\n",
        "texts = df['Tweet']  # List to store your text data (sarcastic and non-sarcastic)\n",
        "labels = df['Sarcasm']  # List to store labels (1 for sarcastic, 0 for non-sarcastic)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the GRU model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(GRU(64, return_sequences=True))  # GRU layer with return_sequences=True\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model (replace with desired evaluation metrics)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new data (optional)\n",
        "new_text = \"This new restaurant is a real gem, NOT.\"  # Replace with your text\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "if prediction > 0.5:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HhyHOfv_fBj",
        "outputId": "8d3ef25d-7ee0-4a4d-ac61-a672d8dc7eeb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 16s 147ms/step - loss: 0.6010 - accuracy: 0.6703 - val_loss: 0.4995 - val_accuracy: 0.7638\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 12s 157ms/step - loss: 0.3599 - accuracy: 0.8419 - val_loss: 0.4702 - val_accuracy: 0.7722\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 10s 137ms/step - loss: 0.1822 - accuracy: 0.9316 - val_loss: 0.6521 - val_accuracy: 0.7404\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 10s 139ms/step - loss: 0.0884 - accuracy: 0.9702 - val_loss: 0.8510 - val_accuracy: 0.7219\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 11s 141ms/step - loss: 0.0494 - accuracy: 0.9857 - val_loss: 0.9966 - val_accuracy: 0.7136\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 0.0384 - accuracy: 0.9895 - val_loss: 1.0526 - val_accuracy: 0.7337\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 10s 135ms/step - loss: 0.0222 - accuracy: 0.9950 - val_loss: 1.1506 - val_accuracy: 0.7370\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 11s 143ms/step - loss: 0.0148 - accuracy: 0.9979 - val_loss: 1.2441 - val_accuracy: 0.7454\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 11s 142ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 1.3899 - val_accuracy: 0.7353\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 11s 142ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 1.4554 - val_accuracy: 0.7370\n",
            "19/19 [==============================] - 1s 27ms/step - loss: 1.4554 - accuracy: 0.7370\n",
            "Test Loss: 1.4554, Accuracy: 0.7370\n",
            "1/1 [==============================] - 1s 674ms/step\n",
            "This text is predicted to be non-sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3)Convolutional Neural Networks (CNNs) with Gated Convolutions"
      ],
      "metadata": {
        "id": "akxjo6SABMMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "max_len = 100  # Maximum sequence length\n",
        "vocab_size = 10000  # Limit on the number of words in vocabulary\n",
        "embedding_dim = 128  # Dimensionality of word embeddings\n",
        "filter_sizes = [3, 4, 5]  # Kernel window sizes for Gated Convolutions\n",
        "num_filters = 64  # Number of filters in the convolutional layers\n",
        "\n",
        "# Load preprocessed data (replace with your actual data)\n",
        "texts = df['Tweet']  # List to store your text data (sarcastic and non-sarcastic)\n",
        "labels = df['Sarcasm']  # List to store labels (1 for sarcastic, 0 for non-sarcastic)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Embedding layer (convert words to vectors)\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_len)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Gated Convolutional layers (with different kernel window sizes)\n",
        "for filter_size in filter_sizes:\n",
        "    conv_layer = Conv1D(num_filters, filter_size, activation='tanh', padding='same')\n",
        "    gated_conv_layer = Conv1D(num_filters, filter_size, activation='sigmoid', padding='same')\n",
        "    model.add(conv_layer)\n",
        "    model.add(gated_conv_layer)\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "# Global Max Pooling layer\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layers for classification\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model (replace with desired evaluation metrics)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new data (optional)\n",
        "new_text = \"This new restaurant is a real gem, NOT.\"  # Replace with your text\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "if prediction > 0.5:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcxomXQZBKzj",
        "outputId": "27c256a9-f89d-49b8-c324-a9603ef398cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 10s 96ms/step - loss: 0.6923 - accuracy: 0.5487 - val_loss: 0.6927 - val_accuracy: 0.5812\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.6924 - accuracy: 0.5457 - val_loss: 0.6885 - val_accuracy: 0.5812\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 6s 83ms/step - loss: 0.6892 - accuracy: 0.5583 - val_loss: 0.6876 - val_accuracy: 0.5812\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.6875 - accuracy: 0.5612 - val_loss: 0.6738 - val_accuracy: 0.5812\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.5669 - accuracy: 0.7290 - val_loss: 0.5671 - val_accuracy: 0.7219\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 8s 111ms/step - loss: 0.3362 - accuracy: 0.8758 - val_loss: 0.5703 - val_accuracy: 0.7286\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 6s 78ms/step - loss: 0.2132 - accuracy: 0.9346 - val_loss: 0.6456 - val_accuracy: 0.7303\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 7s 93ms/step - loss: 0.1499 - accuracy: 0.9581 - val_loss: 0.7055 - val_accuracy: 0.7085\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 6s 82ms/step - loss: 0.1035 - accuracy: 0.9719 - val_loss: 0.7913 - val_accuracy: 0.7270\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 0.0715 - accuracy: 0.9820 - val_loss: 0.8678 - val_accuracy: 0.7253\n",
            "19/19 [==============================] - 0s 18ms/step - loss: 0.8678 - accuracy: 0.7253\n",
            "Test Loss: 0.8678, Accuracy: 0.7253\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "This text is predicted to be non-sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN (Recurrent Neural Networks)"
      ],
      "metadata": {
        "id": "Q4hrzo_wCxgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # For early stopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "max_len = 100\n",
        "vocab_size = 10000\n",
        "embedding_dim = 128\n",
        "texts = df['Tweet']\n",
        "\n",
        "# Handle potential NaN values and ensure labels are within range\n",
        "labels = df['Sarcasm'].map({'sarcasm': 1, 'non-sarcasm': 0}).fillna(0).astype(int)\n",
        "\n",
        "# One-hot encode labels for categorical crossentropy\n",
        "labels = to_categorical(labels, num_classes=2)  # 2 classes (sarcastic, non-sarcastic)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RNN model using SimpleRNN\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(SimpleRNN(64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2, activation='softmax'))  # Output layer with softmax for multiple classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Example prediction\n",
        "new_text = \"This new restaurant is a real gem, NOT.\"\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "predicted_class = prediction.argmax(axis=1)[0]\n",
        "\n",
        "if predicted_class == 1:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhW88i8Uwrc6",
        "outputId": "3385dc61-df6f-41da-82a3-f65d062c6b50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "75/75 [==============================] - 12s 117ms/step - loss: 0.0322 - accuracy: 0.9887 - val_loss: 5.7867e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 6s 85ms/step - loss: 9.0501e-04 - accuracy: 1.0000 - val_loss: 2.8302e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 8s 109ms/step - loss: 5.1742e-04 - accuracy: 1.0000 - val_loss: 1.6981e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 3.5029e-04 - accuracy: 1.0000 - val_loss: 1.1713e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 9s 120ms/step - loss: 2.5257e-04 - accuracy: 1.0000 - val_loss: 8.7502e-05 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 8s 112ms/step - loss: 1.9957e-04 - accuracy: 1.0000 - val_loss: 6.8977e-05 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 8s 111ms/step - loss: 1.5799e-04 - accuracy: 1.0000 - val_loss: 5.6582e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 8s 110ms/step - loss: 1.3179e-04 - accuracy: 1.0000 - val_loss: 4.7611e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 7s 88ms/step - loss: 1.1542e-04 - accuracy: 1.0000 - val_loss: 4.0258e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 8s 113ms/step - loss: 9.7770e-05 - accuracy: 1.0000 - val_loss: 3.4973e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 8.9075e-05 - accuracy: 1.0000 - val_loss: 3.0690e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 7.5396e-05 - accuracy: 1.0000 - val_loss: 2.7314e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 8s 102ms/step - loss: 7.4909e-05 - accuracy: 1.0000 - val_loss: 2.4212e-05 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 8s 100ms/step - loss: 6.1251e-05 - accuracy: 1.0000 - val_loss: 2.1806e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 8s 112ms/step - loss: 5.2860e-05 - accuracy: 1.0000 - val_loss: 1.9881e-05 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 7s 88ms/step - loss: 5.3538e-05 - accuracy: 1.0000 - val_loss: 1.8083e-05 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 4.7604e-05 - accuracy: 1.0000 - val_loss: 1.6535e-05 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 7s 88ms/step - loss: 4.6950e-05 - accuracy: 1.0000 - val_loss: 1.5137e-05 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 8s 101ms/step - loss: 3.8166e-05 - accuracy: 1.0000 - val_loss: 1.3990e-05 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 3.7628e-05 - accuracy: 1.0000 - val_loss: 1.2928e-05 - val_accuracy: 1.0000\n",
            "19/19 [==============================] - 0s 15ms/step - loss: 1.2928e-05 - accuracy: 1.0000\n",
            "Test Loss: 0.0000, Accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 445ms/step\n",
            "This text is predicted to be non-sarcastic.\n"
          ]
        }
      ]
    }
  ]
}