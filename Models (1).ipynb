{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Different Machine Learning Models"
      ],
      "metadata": {
        "id": "ndi8Si3k6ToV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('t_dataset.csv', index_col=False)"
      ],
      "metadata": {
        "id": "soKKbFRi4X1M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Long Short-Term Memory (LSTM) Networks:\n",
        "\n"
      ],
      "metadata": {
        "id": "3BG9XCiy_Uo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# ... rest of your code ...\n",
        "# Hyperparameters (adjust as needed)\n",
        "max_len = 100  # Maximum sequence length\n",
        "vocab_size = 10000  # Limit on the number of words in vocabulary\n",
        "embedding_dim = 128  # Dimensionality of word embeddings\n",
        "\n",
        "# Load preprocessed data\n",
        "texts = df['Tweet']  # List to store your text data (sarcastic and non-sarcastic)\n",
        "labels = df['Sarcasm']  # List to store labels (1 for sarcastic, 0 for non-sarcastic)\n",
        "labels = df['Sarcasm'].map({'yes': 1, 'no': 0})\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Bidirectional LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # Use Bidirectional LSTM\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Train the model\n",
        "# The model definition and compilation needs to happen before calling fit\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model (replace with desired evaluation metrics)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new data (optional)\n",
        "new_text = \"My mom asked me this question as well.\"  # Replace with your text\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "if prediction > 0.5:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euc-achMfSYL",
        "outputId": "055b0a50-9cd7-40a0-adaa-f2b5feed33a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 33s 320ms/step - loss: 0.6064 - accuracy: 0.6854 - val_loss: 0.4891 - val_accuracy: 0.7856\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 20s 273ms/step - loss: 0.3718 - accuracy: 0.8494 - val_loss: 0.4585 - val_accuracy: 0.7990\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 24s 317ms/step - loss: 0.1821 - accuracy: 0.9362 - val_loss: 0.5244 - val_accuracy: 0.7806\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 20s 264ms/step - loss: 0.1030 - accuracy: 0.9677 - val_loss: 0.6071 - val_accuracy: 0.7688\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 22s 294ms/step - loss: 0.0724 - accuracy: 0.9761 - val_loss: 0.7021 - val_accuracy: 0.7772\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 21s 278ms/step - loss: 0.0458 - accuracy: 0.9883 - val_loss: 0.8467 - val_accuracy: 0.7454\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 21s 280ms/step - loss: 0.0326 - accuracy: 0.9899 - val_loss: 0.9280 - val_accuracy: 0.7621\n",
            "19/19 [==============================] - 1s 58ms/step - loss: 0.9280 - accuracy: 0.7621\n",
            "Test Loss: 0.9280, Accuracy: 0.7621\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "This text is predicted to be non-sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Gated Recurrent Units (GRUs):"
      ],
      "metadata": {
        "id": "CeZ5LIhL_aD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "max_len = 100  # Maximum sequence length\n",
        "vocab_size = 10000  # Limit on the number of words in vocabulary\n",
        "embedding_dim = 128  # Dimensionality of word embeddings\n",
        "\n",
        "# Load preprocessed data (replace with your actual data)\n",
        "texts = df['Tweet']  # List to store your text data (sarcastic and non-sarcastic)\n",
        "labels = df['Sarcasm']  # List to store labels (1 for sarcastic, 0 for non-sarcastic)\n",
        "labels = df['Sarcasm'].map({'yes': 1, 'no': 0})\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the GRU model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(GRU(64, return_sequences=True))  # GRU layer with return_sequences=True\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model (replace with desired evaluation metrics)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new data (optional)\n",
        "new_text = \"Sure, let's just add this to my already overflowing to-do list.\"  # Replace with your text\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "if prediction > 0.5:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HhyHOfv_fBj",
        "outputId": "b6def36d-e72f-4b9c-efcb-394a0a37ca2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 19s 166ms/step - loss: 0.5928 - accuracy: 0.6661 - val_loss: 0.4454 - val_accuracy: 0.8023\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 13s 169ms/step - loss: 0.3261 - accuracy: 0.8570 - val_loss: 0.5177 - val_accuracy: 0.7638\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 13s 174ms/step - loss: 0.1656 - accuracy: 0.9346 - val_loss: 0.6420 - val_accuracy: 0.7303\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 11s 145ms/step - loss: 0.0805 - accuracy: 0.9736 - val_loss: 0.9262 - val_accuracy: 0.7370\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 11s 149ms/step - loss: 0.0504 - accuracy: 0.9824 - val_loss: 0.9403 - val_accuracy: 0.7236\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 11s 149ms/step - loss: 0.0354 - accuracy: 0.9899 - val_loss: 1.2228 - val_accuracy: 0.7119\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 11s 154ms/step - loss: 0.0278 - accuracy: 0.9933 - val_loss: 1.1474 - val_accuracy: 0.7337\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 12s 159ms/step - loss: 0.0195 - accuracy: 0.9950 - val_loss: 1.2276 - val_accuracy: 0.7437\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 12s 157ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 1.3915 - val_accuracy: 0.7370\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 9s 125ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 1.2493 - val_accuracy: 0.7203\n",
            "19/19 [==============================] - 0s 24ms/step - loss: 1.2493 - accuracy: 0.7203\n",
            "Test Loss: 1.2493, Accuracy: 0.7203\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "This text is predicted to be sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3)Convolutional Neural Networks (CNNs) with Gated Convolutions"
      ],
      "metadata": {
        "id": "akxjo6SABMMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # For early stopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "max_len = 100\n",
        "vocab_size = 10000\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 64\n",
        "epochs = 10  # Experiment with different values\n",
        "\n",
        "# Load preprocessed data (replace with your actual data)\n",
        "texts = df['Tweet']\n",
        "labels = df['Sarcasm']\n",
        "labels = df['Sarcasm'].map({'yes': 1, 'no': 0}) # Map 'yes' to 1 and 'no' to 0\n",
        "# ... (rest of your code remains mostly unchanged)\n",
        "\n",
        "# Text preprocessing (optional)\n",
        "# ... (add preprocessing steps like lowercasing, punctuation removal, etc.)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Embedding layer\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_len)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Gated Convolutional layers\n",
        "for filter_size in filter_sizes:\n",
        "  conv_layer = Conv1D(num_filters, filter_size, activation='tanh', padding='same')\n",
        "  gated_conv_layer = Conv1D(num_filters, filter_size, activation='sigmoid', padding='same')\n",
        "  model.add(conv_layer)\n",
        "  model.add(gated_conv_layer)\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "# Global Max Pooling layer\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layers for classification\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model (replace with desired evaluation metrics)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new data (optional)\n",
        "new_text = \"Sure, let's just add this to my already overflowing to-do list.\"  # Replace with your text\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "if prediction > 0.5:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXrgNXL9dB7T",
        "outputId": "df1e6dcf-03d1-4b43-c3f8-b1ec8c7a31d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 12s 127ms/step - loss: 0.7000 - accuracy: 0.5352 - val_loss: 0.6868 - val_accuracy: 0.5812\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 9s 116ms/step - loss: 0.6909 - accuracy: 0.5516 - val_loss: 0.6823 - val_accuracy: 0.5812\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 8s 103ms/step - loss: 0.6941 - accuracy: 0.5378 - val_loss: 0.6801 - val_accuracy: 0.5812\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 9s 126ms/step - loss: 0.6877 - accuracy: 0.5612 - val_loss: 0.6837 - val_accuracy: 0.5812\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 7s 93ms/step - loss: 0.6547 - accuracy: 0.6070 - val_loss: 0.5884 - val_accuracy: 0.7052\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 10s 131ms/step - loss: 0.4289 - accuracy: 0.8305 - val_loss: 0.5951 - val_accuracy: 0.6935\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 8s 102ms/step - loss: 0.2776 - accuracy: 0.9002 - val_loss: 0.6273 - val_accuracy: 0.7169\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 9s 121ms/step - loss: 0.1877 - accuracy: 0.9434 - val_loss: 0.6548 - val_accuracy: 0.7203\n",
            "19/19 [==============================] - 0s 18ms/step - loss: 0.6548 - accuracy: 0.7203\n",
            "Test Loss: 0.6548, Accuracy: 0.7203\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "This text is predicted to be sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN (Recurrent Neural Networks)"
      ],
      "metadata": {
        "id": "Q4hrzo_wCxgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # For early stopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "max_len = 100\n",
        "vocab_size = 10000\n",
        "embedding_dim = 128\n",
        "texts = df['Tweet']\n",
        "\n",
        "# Handle potential NaN values and ensure labels are within range\n",
        "labels = df['Sarcasm'].map({'sarcasm': 1, 'non-sarcasm': 0}).fillna(0).astype(int)\n",
        "\n",
        "# One-hot encode labels for categorical crossentropy\n",
        "labels = to_categorical(labels, num_classes=2)  # 2 classes (sarcastic, non-sarcastic)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RNN model using SimpleRNN\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(SimpleRNN(64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2, activation='softmax'))  # Output layer with softmax for multiple classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Example prediction\n",
        "new_text = \"Sure, let's just add this to my already overflowing to-do list.\"\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "prediction = model.predict(padded_sequence)\n",
        "predicted_class = prediction.argmax(axis=1)[0]\n",
        "\n",
        "if predicted_class == 1:\n",
        "    print(\"This text is predicted to be sarcastic.\")\n",
        "else:\n",
        "    print(\"This text is predicted to be non-sarcastic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhW88i8Uwrc6",
        "outputId": "e00026c0-13b2-4cd6-8de5-ac2e12ab13b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "75/75 [==============================] - 13s 115ms/step - loss: 0.0138 - accuracy: 0.9983 - val_loss: 2.6512e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 6s 78ms/step - loss: 4.8192e-04 - accuracy: 1.0000 - val_loss: 1.4829e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 9s 121ms/step - loss: 2.9566e-04 - accuracy: 1.0000 - val_loss: 1.0086e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 7s 93ms/step - loss: 2.1120e-04 - accuracy: 1.0000 - val_loss: 7.4548e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 8s 110ms/step - loss: 1.7449e-04 - accuracy: 1.0000 - val_loss: 5.6506e-05 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 6s 85ms/step - loss: 1.4496e-04 - accuracy: 1.0000 - val_loss: 4.4377e-05 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 8s 103ms/step - loss: 1.0792e-04 - accuracy: 1.0000 - val_loss: 3.6232e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 8.2785e-05 - accuracy: 1.0000 - val_loss: 3.0603e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 7s 99ms/step - loss: 7.9300e-05 - accuracy: 1.0000 - val_loss: 2.5862e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 6s 76ms/step - loss: 7.1096e-05 - accuracy: 1.0000 - val_loss: 2.2045e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 8s 107ms/step - loss: 6.1275e-05 - accuracy: 1.0000 - val_loss: 1.8943e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 7s 93ms/step - loss: 4.7292e-05 - accuracy: 1.0000 - val_loss: 1.6712e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 4.3106e-05 - accuracy: 1.0000 - val_loss: 1.4776e-05 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 7s 100ms/step - loss: 4.0150e-05 - accuracy: 1.0000 - val_loss: 1.3094e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 7s 87ms/step - loss: 3.5615e-05 - accuracy: 1.0000 - val_loss: 1.1669e-05 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 6s 81ms/step - loss: 3.1673e-05 - accuracy: 1.0000 - val_loss: 1.0461e-05 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 7s 97ms/step - loss: 2.9203e-05 - accuracy: 1.0000 - val_loss: 9.4498e-06 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 10s 136ms/step - loss: 2.7969e-05 - accuracy: 1.0000 - val_loss: 8.5012e-06 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 2.2686e-05 - accuracy: 1.0000 - val_loss: 7.7624e-06 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 8s 102ms/step - loss: 2.1088e-05 - accuracy: 1.0000 - val_loss: 7.1138e-06 - val_accuracy: 1.0000\n",
            "19/19 [==============================] - 0s 18ms/step - loss: 7.1138e-06 - accuracy: 1.0000\n",
            "Test Loss: 0.0000, Accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 275ms/step\n",
            "This text is predicted to be non-sarcastic.\n"
          ]
        }
      ]
    }
  ]
}