{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5f4d66-f53a-4c49-a0b6-c94c10fac527",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd4349f-8c0d-4957-8b45-33a293926091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 268ms/step - accuracy: 0.6045 - loss: 0.6500 - val_accuracy: 0.8191 - val_loss: 0.4112\n",
      "Epoch 2/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 272ms/step - accuracy: 0.8293 - loss: 0.3986 - val_accuracy: 0.8057 - val_loss: 0.4099\n",
      "Epoch 3/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 248ms/step - accuracy: 0.9254 - loss: 0.2188 - val_accuracy: 0.7638 - val_loss: 0.5189\n",
      "Epoch 4/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 269ms/step - accuracy: 0.9678 - loss: 0.1095 - val_accuracy: 0.7571 - val_loss: 0.6097\n",
      "Epoch 5/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 254ms/step - accuracy: 0.9843 - loss: 0.0660 - val_accuracy: 0.7722 - val_loss: 0.6776\n",
      "Epoch 6/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 250ms/step - accuracy: 0.9822 - loss: 0.0546 - val_accuracy: 0.7370 - val_loss: 0.8967\n",
      "Epoch 7/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 239ms/step - accuracy: 0.9975 - loss: 0.0185 - val_accuracy: 0.7672 - val_loss: 0.9290\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.7752 - loss: 0.9282\n",
      "Test Loss: 0.9290, Accuracy: 0.7672\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 923ms/step\n",
      "This text is predicted to be non-sarcastic.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\t_dataset.csv', index_col=False)\n",
    "\n",
    "df['Sarcasm'] = df['Sarcasm'].apply(lambda x: 1 if x.lower() == 'yes' else 0)  # Map 'yes' to 1, 'no' to 0\n",
    "\n",
    "max_len = 100  # Maximum sequence length\n",
    "vocab_size = 10000  # Limit on the number of words in vocabulary\n",
    "embedding_dim = 128  # Dimensionality of word embeddings\n",
    "\n",
    "texts = df['Tweet'].astype(str)\n",
    "labels = df['Sarcasm'].astype(int)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "new_text = \"This new restaurant is a real gem, NOT.\"\n",
    "sequence = tokenizer.texts_to_sequences([new_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "prediction = model.predict(padded_sequence)\n",
    "if prediction > 0.5:\n",
    "    print(\"This text is predicted to be sarcastic.\")\n",
    "else:\n",
    "    print(\"This text is predicted to be non-sarcastic.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310c76b-2bdb-4b12-9a17-157198a448d6",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5fd24a9-b834-4655-9095-295e015f27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 240ms/step - accuracy: 0.6076 - loss: 0.6484 - val_accuracy: 0.7906 - val_loss: 0.4510\n",
      "Epoch 2/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.8507 - loss: 0.3423 - val_accuracy: 0.7655 - val_loss: 0.4973\n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 178ms/step - accuracy: 0.9293 - loss: 0.1914 - val_accuracy: 0.7554 - val_loss: 0.5683\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 196ms/step - accuracy: 0.9721 - loss: 0.0915 - val_accuracy: 0.7454 - val_loss: 0.7832\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 199ms/step - accuracy: 0.9921 - loss: 0.0357 - val_accuracy: 0.7219 - val_loss: 1.1292\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 209ms/step - accuracy: 0.9931 - loss: 0.0255 - val_accuracy: 0.7387 - val_loss: 1.0466\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 196ms/step - accuracy: 0.9913 - loss: 0.0295 - val_accuracy: 0.7320 - val_loss: 1.1918\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 211ms/step - accuracy: 0.9940 - loss: 0.0159 - val_accuracy: 0.7303 - val_loss: 1.3171\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 201ms/step - accuracy: 0.9995 - loss: 0.0030 - val_accuracy: 0.7270 - val_loss: 1.4244\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 181ms/step - accuracy: 0.9999 - loss: 0.0027 - val_accuracy: 0.7152 - val_loss: 1.6125\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.6995 - loss: 1.7611\n",
      "Test Loss: 1.6125, Accuracy: 0.7152\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
      "This text is predicted to be non-sarcastic.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "\n",
    "max_len = 100  \n",
    "vocab_size = 10000  \n",
    "embedding_dim = 128  \n",
    "\n",
    "texts = df['Tweet'] \n",
    "labels = df['Sarcasm']  \n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GRU(64, return_sequences=True))  \n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "new_text = \"This new restaurant is a real gem, NOT.\" \n",
    "sequence = tokenizer.texts_to_sequences([new_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "prediction = model.predict(padded_sequence)\n",
    "if prediction > 0.5:\n",
    "    print(\"This text is predicted to be sarcastic.\")\n",
    "else:\n",
    "    print(\"This text is predicted to be non-sarcastic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24b3027-38cd-4697-b38f-ee71286503ae",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bd3db0e-9d81-4d8c-8f25-4c628a933819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - accuracy: 0.5325 - loss: 0.7092 - val_accuracy: 0.5812 - val_loss: 0.6817\n",
      "Epoch 2/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 116ms/step - accuracy: 0.5040 - loss: 0.7060 - val_accuracy: 0.5812 - val_loss: 0.6853\n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.5543 - loss: 0.6906 - val_accuracy: 0.5812 - val_loss: 0.6827\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 109ms/step - accuracy: 0.5687 - loss: 0.6860 - val_accuracy: 0.5812 - val_loss: 0.6922\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.5624 - loss: 0.6888 - val_accuracy: 0.5812 - val_loss: 0.6809\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 109ms/step - accuracy: 0.5857 - loss: 0.6733 - val_accuracy: 0.6884 - val_loss: 0.6004\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - accuracy: 0.8517 - loss: 0.4078 - val_accuracy: 0.7353 - val_loss: 0.5593\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.9088 - loss: 0.2747 - val_accuracy: 0.7420 - val_loss: 0.6212\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.9448 - loss: 0.1878 - val_accuracy: 0.7085 - val_loss: 0.7129\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 127ms/step - accuracy: 0.9485 - loss: 0.1478 - val_accuracy: 0.7370 - val_loss: 0.7066\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7228 - loss: 0.7479\n",
      "Test Loss: 0.7066, Accuracy: 0.7370\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001946AEC9580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "This text is predicted to be sarcastic.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "max_len = 100 \n",
    "vocab_size = 10000 \n",
    "embedding_dim = 128\n",
    "filter_sizes = [3, 4, 5] \n",
    "num_filters = 64 \n",
    "\n",
    "texts = df['Tweet']  \n",
    "labels = df['Sarcasm'] \n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_len)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = Conv1D(num_filters, filter_size, activation='tanh', padding='same')\n",
    "    gated_conv_layer = Conv1D(num_filters, filter_size, activation='sigmoid', padding='same')\n",
    "    model.add(conv_layer)\n",
    "    model.add(gated_conv_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "new_text = \"This new restaurant is a real gem, NOT.\"  # Replace with your text\n",
    "sequence = tokenizer.texts_to_sequences([new_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "prediction = model.predict(padded_sequence)\n",
    "if prediction > 0.5:\n",
    "    print(\"This text is predicted to be sarcastic.\")\n",
    "else:\n",
    "    print(\"This text is predicted to be non-sarcastic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30488ef-70b4-41a8-8ab7-f95aad3a2fb3",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c01f2295-108e-4094-ba97-859d15a8f560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 183ms/step - accuracy: 0.9960 - loss: 0.1657 - val_accuracy: 1.0000 - val_loss: 2.7129e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 1.0000 - loss: 3.8461e-04 - val_accuracy: 1.0000 - val_loss: 1.2580e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 1.9432e-04 - val_accuracy: 1.0000 - val_loss: 8.0815e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 201ms/step - accuracy: 1.0000 - loss: 1.3744e-04 - val_accuracy: 1.0000 - val_loss: 5.8036e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 1.0045e-04 - val_accuracy: 1.0000 - val_loss: 4.2834e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 7.1472e-05 - val_accuracy: 1.0000 - val_loss: 3.2464e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 6.4735e-05 - val_accuracy: 1.0000 - val_loss: 2.5937e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 4.9145e-05 - val_accuracy: 1.0000 - val_loss: 2.2073e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 208ms/step - accuracy: 1.0000 - loss: 4.4926e-05 - val_accuracy: 1.0000 - val_loss: 1.9285e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 4.0257e-05 - val_accuracy: 1.0000 - val_loss: 1.7067e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 3.3825e-05 - val_accuracy: 1.0000 - val_loss: 1.5361e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 3.2936e-05 - val_accuracy: 1.0000 - val_loss: 1.3895e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 3.0372e-05 - val_accuracy: 1.0000 - val_loss: 1.2583e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 2.5847e-05 - val_accuracy: 1.0000 - val_loss: 1.1537e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 2.4888e-05 - val_accuracy: 1.0000 - val_loss: 1.0580e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 2.5097e-05 - val_accuracy: 1.0000 - val_loss: 9.6733e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 2.8778e-05 - val_accuracy: 1.0000 - val_loss: 8.9397e-06\n",
      "Epoch 18/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 2.1841e-05 - val_accuracy: 1.0000 - val_loss: 8.2280e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 1.7778e-05 - val_accuracy: 1.0000 - val_loss: 7.6300e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 1.6432e-05 - val_accuracy: 1.0000 - val_loss: 7.1523e-06\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 7.1524e-06\n",
      "Test Loss: 0.0000, Accuracy: 1.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550ms/step\n",
      "This text is predicted to be non-sarcastic.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical  # For categorical crossentropy\n",
    "\n",
    "max_len = 100\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "texts = df['Tweet']\n",
    "\n",
    "labels = df['Sarcasm'].map({'sarcasm': 1, 'non-sarcasm': 0}).fillna(0).astype(int) \n",
    "\n",
    "labels = to_categorical(labels, num_classes=2) \n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the RNN model (consider Bidirectional LSTM or hyperparameter tuning)\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(64, return_sequences=True)) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))  # Experiment with more epochs\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "new_text = \"This new restaurant is a real gem, NOT.\"\n",
    "sequence = tokenizer.texts_to_sequences([new_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "prediction = model.predict(padded_sequence)\n",
    "predicted_class = prediction.argmax(axis=1)[0]  \n",
    "\n",
    "if predicted_class == 1:\n",
    "  print(\"This text is predicted to be sarcastic.\")\n",
    "else:\n",
    "  print(\"This text is predicted to be non-sarcastic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70717175-0b6e-449a-ae7c-05f24f39503a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
