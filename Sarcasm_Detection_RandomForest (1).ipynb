{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ff340d-3286-47ad-ac0e-9256b755b5a1",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddca1e2d-470d-4bfe-a590-9b7edd31f1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>I loovee when people text back  unamused_face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>Don't you love it when your parents are Pissed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>So many useless classes , great to be student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>Oh how I love getting home from work at am and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>I just love having grungy ass hair expressionl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                             Tweets\n",
       "0  sarcastic     I loovee when people text back  unamused_face \n",
       "1  sarcastic  Don't you love it when your parents are Pissed...\n",
       "2  sarcastic      So many useless classes , great to be student\n",
       "3  sarcastic  Oh how I love getting home from work at am and...\n",
       "4  sarcastic  I just love having grungy ass hair expressionl..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572e4fb-ee7e-445a-b095-4e50791ccb96",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a34a3c80-f268-4889-9c29-83f735726a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label                                             Tweets  \\\n",
      "0  sarcastic     I loovee when people text back  unamused_face    \n",
      "1  sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2  sarcastic      So many useless classes , great to be student   \n",
      "3  sarcastic  Oh how I love getting home from work at am and...   \n",
      "4  sarcastic  I just love having grungy ass hair expressionl...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [I, loovee, when, people, text, back, unamused...  \n",
      "1  [Don't, you, love, it, when, your, parents, ar...  \n",
      "2  [So, many, useless, classes, ,, great, to, be,...  \n",
      "3  [Oh, how, I, love, getting, home, from, work, ...  \n",
      "4  [I, just, love, having, grungy, ass, hair, exp...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Initialize the TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(Tweets):\n",
    "    return tokenizer.tokenize(Tweets)\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "df['tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "print(df.head(5))\n",
    "df.to_csv('tokenized_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac01489-dd10-4c69-8a70-11f42ae463e2",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b944934e-1618-40d2-a619-14f5f035bf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label                                             Tweets  \\\n",
      "0  sarcastic     I loovee when people text back  unamused_face    \n",
      "1  sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2  sarcastic      So many useless classes , great to be student   \n",
      "3  sarcastic  Oh how I love getting home from work at am and...   \n",
      "4  sarcastic  I just love having grungy ass hair expressionl...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  [I, loovee, when, people, text, back, unamused...   \n",
      "1  [Don't, you, love, it, when, your, parents, ar...   \n",
      "2  [So, many, useless, classes, ,, great, to, be,...   \n",
      "3  [Oh, how, I, love, getting, home, from, work, ...   \n",
      "4  [I, just, love, having, grungy, ass, hair, exp...   \n",
      "\n",
      "                                     sentence_tokens  \n",
      "0    [I loovee when people text back  unamused_face]  \n",
      "1  [Don't you love it when your parents are Pisse...  \n",
      "2    [So many useless classes , great to be student]  \n",
      "3  [Oh how I love getting home from work at am an...  \n",
      "4  [I just love having grungy ass hair expression...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "\n",
    "# Initialize the TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "    \n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "# Tokenize tweets into words\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "# Tokenize tweets into sentences\n",
    "df['sentence_tokens'] = df['Tweets'].apply(tokenize_sentences)\n",
    "print(df.head(5))\n",
    "df.to_csv('tokenized_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196c00d-6681-4b6f-abd7-aec494b4274a",
   "metadata": {},
   "source": [
    "## Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9fb6bdf-a0a0-4d26-948e-232a7f73e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Label                                             Tweets  \\\n",
      "0   sarcastic     I loovee when people text back  unamused_face    \n",
      "1   sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2   sarcastic      So many useless classes , great to be student   \n",
      "3   sarcastic  Oh how I love getting home from work at am and...   \n",
      "4   sarcastic  I just love having grungy ass hair expressionl...   \n",
      "5   sarcastic  Thank you , random guy , for sneaking up behin...   \n",
      "6   sarcastic  Being half spanish and not being able to speak...   \n",
      "7   sarcastic  I know no one will remember it broken_heart   ...   \n",
      "8   sarcastic  Anyone in Chem that has to do OWLs knows my pa...   \n",
      "9   sarcastic                          Holy crap I look great      \n",
      "10  sarcastic      I love doing 20 sprints in 10 degree weather    \n",
      "11  sarcastic  feeling like a million bucks after that chem 2...   \n",
      "12  sarcastic  I love working in Sydney river It makes me wan...   \n",
      "13  sarcastic  I love hearing things about me that I didn't k...   \n",
      "14  sarcastic  I'm gonna let it slide today , but next time t...   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [I, loovee, when, people, text, back, unamused...   \n",
      "1   [Don't, you, love, it, when, your, parents, ar...   \n",
      "2   [So, many, useless, classes, ,, great, to, be,...   \n",
      "3   [Oh, how, I, love, getting, home, from, work, ...   \n",
      "4   [I, just, love, having, grungy, ass, hair, exp...   \n",
      "5   [Thank, you, ,, random, guy, ,, for, sneaking,...   \n",
      "6   [Being, half, spanish, and, not, being, able, ...   \n",
      "7   [I, know, no, one, will, remember, it, broken_...   \n",
      "8   [Anyone, in, Chem, that, has, to, do, OWLs, kn...   \n",
      "9                        [Holy, crap, I, look, great]   \n",
      "10  [I, love, doing, 20, sprints, in, 10, degree, ...   \n",
      "11  [feeling, like, a, million, bucks, after, that...   \n",
      "12  [I, love, working, in, Sydney, river, It, make...   \n",
      "13  [I, love, hearing, things, about, me, that, I,...   \n",
      "14  [I'm, gonna, let, it, slide, today, ,, but, ne...   \n",
      "\n",
      "                                      sentence_tokens  \\\n",
      "0     [I loovee when people text back  unamused_face]   \n",
      "1   [Don't you love it when your parents are Pisse...   \n",
      "2     [So many useless classes , great to be student]   \n",
      "3   [Oh how I love getting home from work at am an...   \n",
      "4   [I just love having grungy ass hair expression...   \n",
      "5   [Thank you , random guy , for sneaking up behi...   \n",
      "6   [Being half spanish and not being able to spea...   \n",
      "7       [I know no one will remember it broken_heart]   \n",
      "8   [Anyone in Chem that has to do OWLs knows my p...   \n",
      "9                            [Holy crap I look great]   \n",
      "10     [I love doing 20 sprints in 10 degree weather]   \n",
      "11  [feeling like a million bucks after that chem ...   \n",
      "12  [I love working in Sydney river It makes me wa...   \n",
      "13  [I love hearing things about me that I didn't ...   \n",
      "14  [I'm gonna let it slide today , but next time ...   \n",
      "\n",
      "                                          char_tokens  \n",
      "0   [I,  , l, o, o, v, e, e,  , w, h, e, n,  , p, ...  \n",
      "1   [D, o, n, ', t,  , y, o, u,  , l, o, v, e,  , ...  \n",
      "2   [S, o,  , m, a, n, y,  , u, s, e, l, e, s, s, ...  \n",
      "3   [O, h,  , h, o, w,  , I,  , l, o, v, e,  , g, ...  \n",
      "4   [I,  , j, u, s, t,  , l, o, v, e,  , h, a, v, ...  \n",
      "5   [T, h, a, n, k,  , y, o, u,  , ,,  , r, a, n, ...  \n",
      "6   [B, e, i, n, g,  , h, a, l, f,  , s, p, a, n, ...  \n",
      "7   [I,  , k, n, o, w,  , n, o,  , o, n, e,  , w, ...  \n",
      "8   [A, n, y, o, n, e,  , i, n,  , C, h, e, m,  , ...  \n",
      "9   [H, o, l, y,  , c, r, a, p,  , I,  , l, o, o, ...  \n",
      "10  [I,  , l, o, v, e,  , d, o, i, n, g,  , 2, 0, ...  \n",
      "11  [f, e, e, l, i, n, g,  , l, i, k, e,  , a,  , ...  \n",
      "12  [I,  , l, o, v, e,  , w, o, r, k, i, n, g,  , ...  \n",
      "13  [I,  , l, o, v, e,  , h, e, a, r, i, n, g,  , ...  \n",
      "14  [I, ', m,  , g, o, n, n, a,  , l, e, t,  , i, ...  \n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize text into words using TweetTokenizer\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "# sent_tokenize\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# characters\n",
    "def tokenize_characters(text):\n",
    "    return list(text) \n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweets'].apply(tokenize_sentences)\n",
    "\n",
    "df['char_tokens'] = df['Tweets'].apply(tokenize_characters)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\tokenized_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e460df-2872-43a2-8e3b-a6d92e4858ff",
   "metadata": {},
   "source": [
    "## Sub word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30667a43-8d70-4268-ad1d-707f83ed6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label                                             Tweets  \\\n",
      "0  sarcastic     I loovee when people text back  unamused_face    \n",
      "1  sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2  sarcastic      So many useless classes , great to be student   \n",
      "3  sarcastic  Oh how I love getting home from work at am and...   \n",
      "4  sarcastic  I just love having grungy ass hair expressionl...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  [i, lo, ##ove, ##e, when, people, text, back, ...   \n",
      "1  [don, ', t, you, love, it, when, your, parents...   \n",
      "2  [so, many, useless, classes, ,, great, to, be,...   \n",
      "3  [oh, how, i, love, getting, home, from, work, ...   \n",
      "4  [i, just, love, having, gr, ##ung, ##y, ass, h...   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  [I,  , l, o, o, v, e, e,  , w, h, e, n,  , p, ...   \n",
      "1  [D, o, n, ', t,  , y, o, u,  , l, o, v, e,  , ...   \n",
      "2  [S, o,  , m, a, n, y,  , u, s, e, l, e, s, s, ...   \n",
      "3  [O, h,  , h, o, w,  , I,  , l, o, v, e,  , g, ...   \n",
      "4  [I,  , j, u, s, t,  , l, o, v, e,  , h, a, v, ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0    [I loovee when people text back  unamused_face]   \n",
      "1  [Don't you love it when your parents are Pisse...   \n",
      "2    [So many useless classes , great to be student]   \n",
      "3  [Oh how I love getting home from work at am an...   \n",
      "4  [I just love having grungy ass hair expression...   \n",
      "\n",
      "                                      subword_tokens  \n",
      "0  [i, lo, ##ove, ##e, when, people, text, back, ...  \n",
      "1  [don, ', t, you, love, it, when, your, parents...  \n",
      "2  [so, many, useless, classes, ,, great, to, be,...  \n",
      "3  [oh, how, i, love, getting, home, from, work, ...  \n",
      "4  [i, just, love, having, gr, ##ung, ##y, ass, h...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_subwords(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweets'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweets'].apply(tokenize_sentences)\n",
    "\n",
    "df['subword_tokens'] = df['Tweets'].apply(tokenize_subwords)\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "df.to_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\tokenized_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb0820-81fe-4199-b3b7-05ab7d6cf23e",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "230dcd96-19a1-41d0-ac8c-5538928410be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label                                             Tweets  \\\n",
      "0  sarcastic     I loovee when people text back  unamused_face    \n",
      "1  sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2  sarcastic      So many useless classes , great to be student   \n",
      "3  sarcastic  Oh how I love getting home from work at am and...   \n",
      "4  sarcastic  I just love having grungy ass hair expressionl...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  [I, loovee, when, people, text, back, unamused...   \n",
      "1  [Don't, you, love, it, when, your, parents, ar...   \n",
      "2  [So, many, useless, classes, ,, great, to, be,...   \n",
      "3  [Oh, how, I, love, getting, home, from, work, ...   \n",
      "4  [I, just, love, having, grungy, ass, hair, exp...   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  [I,  , l, o, o, v, e, e,  , w, h, e, n,  , p, ...   \n",
      "1  [D, o, n, ', t,  , y, o, u,  , l, o, v, e,  , ...   \n",
      "2  [S, o,  , m, a, n, y,  , u, s, e, l, e, s, s, ...   \n",
      "3  [O, h,  , h, o, w,  , I,  , l, o, v, e,  , g, ...   \n",
      "4  [I,  , j, u, s, t,  , l, o, v, e,  , h, a, v, ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0    [I loovee when people text back  unamused_face]   \n",
      "1  [Don't you love it when your parents are Pisse...   \n",
      "2    [So many useless classes , great to be student]   \n",
      "3  [Oh how I love getting home from work at am an...   \n",
      "4  [I just love having grungy ass hair expression...   \n",
      "\n",
      "                                      subword_tokens  \\\n",
      "0  [I, loovee, when, people, text, back, unamused...   \n",
      "1  [Don't, you, love, it, when, your, parents, ar...   \n",
      "2  [So, many, useless, classes, ,, great, to, be,...   \n",
      "3  [Oh, how, I, love, getting, home, from, work, ...   \n",
      "4  [I, just, love, having, grungy, ass, hair, exp...   \n",
      "\n",
      "                                   lemmatized_tokens  \n",
      "0  [I, loovee, when, people, text, back, unamused...  \n",
      "1  [Don't, you, love, it, when, your, parent, be,...  \n",
      "2  [So, many, useless, class, ,, great, to, be, s...  \n",
      "3  [Oh, how, I, love, get, home, from, work, at, ...  \n",
      "4  [I, just, love, have, grungy, as, hair, expres...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize words\n",
    "def lemmatize_words(tokens):\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag([token])[0][1][0].upper() \n",
    "        pos_tag = pos_tag if pos_tag in ['A', 'N', 'V'] else 'n' \n",
    "        lemma_tokens.append(lemmatizer.lemmatize(token, pos=pos_tag.lower()))\n",
    "    \n",
    "    return lemma_tokens\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenizer.tokenize)\n",
    "\n",
    "df['char_tokens'] = df['Tweets'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweets'].apply(tokenize_sentences)\n",
    "\n",
    "df['subword_tokens'] = df['Tweets'].apply(tokenize_subwords)\n",
    "\n",
    "df['lemmatized_tokens'] = df['word_tokens'].apply(lemmatize_words)\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "df.to_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\tokenized_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0d156-c0b6-48d7-ba71-032a7979eb22",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d7492cb-238d-4f68-b6c2-66cff1b525db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\tokenized_dataset.csv\")\n",
    "df_encoded = pd.get_dummies(df, columns=['Label'])\n",
    "\n",
    "df_encoded.to_csv(\"onehotencoded_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2441a0d7-e289-4d6b-b395-eb446595585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8886075949367088\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89       194\n",
      "           1       0.89      0.90      0.89       201\n",
      "\n",
      "    accuracy                           0.89       395\n",
      "   macro avg       0.89      0.89      0.89       395\n",
      "weighted avg       0.89      0.89      0.89       395\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171  23]\n",
      " [ 21 180]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['Clean_Tweets'] = df['Tweets'].apply(preprocess)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Clean_Tweets'])\n",
    "\n",
    "y = df['Label'].apply(lambda x: 1 if x == 'sarcastic' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b737f5e-29ca-47aa-b7dc-df8bdce39f3c",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d99f1d0f-65fa-45e8-aa7c-626060f334ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8962025316455696\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89       194\n",
      "           1       0.87      0.94      0.90       201\n",
      "\n",
      "    accuracy                           0.90       395\n",
      "   macro avg       0.90      0.90      0.90       395\n",
      "weighted avg       0.90      0.90      0.90       395\n",
      "\n",
      "Confusion Matrix:\n",
      "[[166  28]\n",
      " [ 13 188]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "df['Clean_Tweets'] = df['Tweets'].apply(preprocess)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['Clean_Tweets'])\n",
    "\n",
    "y = df['Label'].apply(lambda x: 1 if x == 'sarcastic' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "final_df = pd.concat([df.drop(['Tweets', 'Clean_Tweets'], axis=1), tfidf_df], axis=1)\n",
    "final_df.to_csv(\"tfidf_encoded_tweets.csv\", index=False)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1e272-e230-4595-aea0-1df7661c68b3",
   "metadata": {},
   "source": [
    "## LABEL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51dcee75-9a74-43d2-8bb8-670f290bc110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8886075949367088\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89       194\n",
      "           1       0.89      0.90      0.89       201\n",
      "\n",
      "    accuracy                           0.89       395\n",
      "   macro avg       0.89      0.89      0.89       395\n",
      "weighted avg       0.89      0.89      0.89       395\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171  23]\n",
      " [ 21 180]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['Clean_Tweets'] = df['Tweets'].apply(preprocess)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(df['Clean_Tweets'])\n",
    "\n",
    "y = df['Label'].apply(lambda x: 1 if x == 'sarcastic' else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "count_df = pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "final_df = pd.concat([df.drop(['Tweets', 'Clean_Tweets'], axis=1), count_df], axis=1)\n",
    "final_df.to_csv(\"count_encoded_tweets.csv\", index=False)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba1489-767a-412b-b848-0958bcd3a0b6",
   "metadata": {},
   "source": [
    "## Term Frequency Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c724be04-68ff-4e8d-a6ff-823c4df5ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8886075949367088\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89       194\n",
      "           1       0.89      0.90      0.89       201\n",
      "\n",
      "    accuracy                           0.89       395\n",
      "   macro avg       0.89      0.89      0.89       395\n",
      "weighted avg       0.89      0.89      0.89       395\n",
      "\n",
      "Confusion Matrix:\n",
      "[[171  23]\n",
      " [ 21 180]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['Clean_Tweets'] = df['Tweets'].apply(preprocess)\n",
    "\n",
    "tf_vectorizer = CountVectorizer()\n",
    "X = tf_vectorizer.fit_transform(df['Clean_Tweets'])\n",
    "\n",
    "y = df['Label'].apply(lambda x: 1 if x == 'sarcastic' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "tf_df = pd.DataFrame(X.toarray(), columns=tf_vectorizer.get_feature_names_out())\n",
    "final_df = pd.concat([df.drop(['Tweets', 'Clean_Tweets'], axis=1), tf_df], axis=1)\n",
    "final_df.to_csv(\"tf_encoded_tweets.csv\", index=False)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d3564-2692-4387-a8b7-a7fbe252f21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
