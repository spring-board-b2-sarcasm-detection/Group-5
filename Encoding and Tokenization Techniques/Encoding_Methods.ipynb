{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779e82b3-6415-4b81-b6d7-885ed60ef7b3",
   "metadata": {},
   "source": [
    "# Encoding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c78387-3860-47c3-b8b3-514d72649500",
   "metadata": {},
   "source": [
    "In data preprocessing, encoding methods refer to techniques used to convert categorical data into a numerical format that can be used for machine learning algorithms. Categorical data represents types of data which may be divided into groups, and categorical data is often represented by words rather than numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5223d02-bdb3-424a-9efa-b5a26e7f82e6",
   "metadata": {},
   "source": [
    "### One Hot Encoder:\n",
    "Converts categorical variables into binary vectors where each vector represents a category with a 1 and all others with 0s.\n",
    "\n",
    "### Label Encoder: \n",
    "Converts categorical variables into integer labels, assigning a unique number to each category.\n",
    "\n",
    "### TF-IDF (Term Frequency-Inverse Document Frequency): \n",
    "Represents documents as vectors based on word importance, where high-frequency terms across documents are weighted lower.\n",
    "\n",
    "### Word2Vec: \n",
    "Represents words as dense vectors in a continuous space, capturing semantic relationships between words based on their usage contexts.\n",
    "\n",
    "### Term Frequency Encoder: \n",
    "Represents documents as vectors based on term frequencies, where each element in the vector corresponds to the frequency of a term in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480cc91-8dba-42b3-8516-be5207789711",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecc2cd-253a-4f93-82ea-c887d09a14cf",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f671ed1-207e-4366-ac79-32eb386b2671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>char_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>subword_tokens</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>I loovee when people text back  unamused_face</td>\n",
       "      <td>['i', 'lo', '##ove', '##e', 'when', 'people', ...</td>\n",
       "      <td>['I', ' ', 'l', 'o', 'o', 'v', 'e', 'e', ' ', ...</td>\n",
       "      <td>['I loovee when people text back  unamused_face']</td>\n",
       "      <td>['i', 'lo', '##ove', '##e', 'when', 'people', ...</td>\n",
       "      <td>I loovee when people text back unamused_face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>Don't you love it when your parents are Pissed...</td>\n",
       "      <td>['don', \"'\", 't', 'you', 'love', 'it', 'when',...</td>\n",
       "      <td>['D', 'o', 'n', \"'\", 't', ' ', 'y', 'o', 'u', ...</td>\n",
       "      <td>[\"Don't you love it when your parents are Piss...</td>\n",
       "      <td>['don', \"'\", 't', 'you', 'love', 'it', 'when',...</td>\n",
       "      <td>Do n't you love it when your parent are Pissed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>So many useless classes , great to be student</td>\n",
       "      <td>['so', 'many', 'useless', 'classes', ',', 'gre...</td>\n",
       "      <td>['S', 'o', ' ', 'm', 'a', 'n', 'y', ' ', 'u', ...</td>\n",
       "      <td>['So many useless classes , great to be student']</td>\n",
       "      <td>['so', 'many', 'useless', 'classes', ',', 'gre...</td>\n",
       "      <td>So many useless class , great to be student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>Oh how I love getting home from work at am and...</td>\n",
       "      <td>['oh', 'how', 'i', 'love', 'getting', 'home', ...</td>\n",
       "      <td>['O', 'h', ' ', 'h', 'o', 'w', ' ', 'I', ' ', ...</td>\n",
       "      <td>['Oh how I love getting home from work at am a...</td>\n",
       "      <td>['oh', 'how', 'i', 'love', 'getting', 'home', ...</td>\n",
       "      <td>Oh how I love getting home from work at am and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>I just love having grungy ass hair expressionl...</td>\n",
       "      <td>['i', 'just', 'love', 'having', 'gr', '##ung',...</td>\n",
       "      <td>['I', ' ', 'j', 'u', 's', 't', ' ', 'l', 'o', ...</td>\n",
       "      <td>['I just love having grungy ass hair expressio...</td>\n",
       "      <td>['i', 'just', 'love', 'having', 'gr', '##ung',...</td>\n",
       "      <td>I just love having grungy as hair expressionle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                             Tweets  \\\n",
       "0  sarcastic     I loovee when people text back  unamused_face    \n",
       "1  sarcastic  Don't you love it when your parents are Pissed...   \n",
       "2  sarcastic      So many useless classes , great to be student   \n",
       "3  sarcastic  Oh how I love getting home from work at am and...   \n",
       "4  sarcastic  I just love having grungy ass hair expressionl...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  ['i', 'lo', '##ove', '##e', 'when', 'people', ...   \n",
       "1  ['don', \"'\", 't', 'you', 'love', 'it', 'when',...   \n",
       "2  ['so', 'many', 'useless', 'classes', ',', 'gre...   \n",
       "3  ['oh', 'how', 'i', 'love', 'getting', 'home', ...   \n",
       "4  ['i', 'just', 'love', 'having', 'gr', '##ung',...   \n",
       "\n",
       "                                         char_tokens  \\\n",
       "0  ['I', ' ', 'l', 'o', 'o', 'v', 'e', 'e', ' ', ...   \n",
       "1  ['D', 'o', 'n', \"'\", 't', ' ', 'y', 'o', 'u', ...   \n",
       "2  ['S', 'o', ' ', 'm', 'a', 'n', 'y', ' ', 'u', ...   \n",
       "3  ['O', 'h', ' ', 'h', 'o', 'w', ' ', 'I', ' ', ...   \n",
       "4  ['I', ' ', 'j', 'u', 's', 't', ' ', 'l', 'o', ...   \n",
       "\n",
       "                                     sentence_tokens  \\\n",
       "0  ['I loovee when people text back  unamused_face']   \n",
       "1  [\"Don't you love it when your parents are Piss...   \n",
       "2  ['So many useless classes , great to be student']   \n",
       "3  ['Oh how I love getting home from work at am a...   \n",
       "4  ['I just love having grungy ass hair expressio...   \n",
       "\n",
       "                                      subword_tokens  \\\n",
       "0  ['i', 'lo', '##ove', '##e', 'when', 'people', ...   \n",
       "1  ['don', \"'\", 't', 'you', 'love', 'it', 'when',...   \n",
       "2  ['so', 'many', 'useless', 'classes', ',', 'gre...   \n",
       "3  ['oh', 'how', 'i', 'love', 'getting', 'home', ...   \n",
       "4  ['i', 'just', 'love', 'having', 'gr', '##ung',...   \n",
       "\n",
       "                                    lemmatized_tweet  \n",
       "0       I loovee when people text back unamused_face  \n",
       "1  Do n't you love it when your parent are Pissed...  \n",
       "2        So many useless class , great to be student  \n",
       "3  Oh how I love getting home from work at am and...  \n",
       "4  I just love having grungy as hair expressionle...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tokenized_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a040a9-5868-429f-b65f-d79b52465f65",
   "metadata": {},
   "source": [
    "## One Hot Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3ef106b-3f51-4151-be98-1f2b0ee3a5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Tweets  \\\n",
      "0     I loovee when people text back  unamused_face    \n",
      "1  Don't you love it when your parents are Pissed...   \n",
      "2      So many useless classes , great to be student   \n",
      "3  Oh how I love getting home from work at am and...   \n",
      "4  I just love having grungy ass hair expressionl...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  ['i', 'lo', '##ove', '##e', 'when', 'people', ...   \n",
      "1  ['don', \"'\", 't', 'you', 'love', 'it', 'when',...   \n",
      "2  ['so', 'many', 'useless', 'classes', ',', 'gre...   \n",
      "3  ['oh', 'how', 'i', 'love', 'getting', 'home', ...   \n",
      "4  ['i', 'just', 'love', 'having', 'gr', '##ung',...   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  ['I', ' ', 'l', 'o', 'o', 'v', 'e', 'e', ' ', ...   \n",
      "1  ['D', 'o', 'n', \"'\", 't', ' ', 'y', 'o', 'u', ...   \n",
      "2  ['S', 'o', ' ', 'm', 'a', 'n', 'y', ' ', 'u', ...   \n",
      "3  ['O', 'h', ' ', 'h', 'o', 'w', ' ', 'I', ' ', ...   \n",
      "4  ['I', ' ', 'j', 'u', 's', 't', ' ', 'l', 'o', ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0  ['I loovee when people text back  unamused_face']   \n",
      "1  [\"Don't you love it when your parents are Piss...   \n",
      "2  ['So many useless classes , great to be student']   \n",
      "3  ['Oh how I love getting home from work at am a...   \n",
      "4  ['I just love having grungy ass hair expressio...   \n",
      "\n",
      "                                      subword_tokens  \\\n",
      "0  ['i', 'lo', '##ove', '##e', 'when', 'people', ...   \n",
      "1  ['don', \"'\", 't', 'you', 'love', 'it', 'when',...   \n",
      "2  ['so', 'many', 'useless', 'classes', ',', 'gre...   \n",
      "3  ['oh', 'how', 'i', 'love', 'getting', 'home', ...   \n",
      "4  ['i', 'just', 'love', 'having', 'gr', '##ung',...   \n",
      "\n",
      "                                    lemmatized_tweet  Label_not sarcastic  \\\n",
      "0       I loovee when people text back unamused_face                False   \n",
      "1  Do n't you love it when your parent are Pissed...                False   \n",
      "2        So many useless class , great to be student                False   \n",
      "3  Oh how I love getting home from work at am and...                False   \n",
      "4  I just love having grungy as hair expressionle...                False   \n",
      "\n",
      "   Label_sarcastic  \n",
      "0             True  \n",
      "1             True  \n",
      "2             True  \n",
      "3             True  \n",
      "4             True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tokenized_dataset.csv')\n",
    "\n",
    "# Perform one-hot encoding on the 'Label' column\n",
    "df_encoded = pd.get_dummies(df, columns=['Label'])\n",
    "\n",
    "print(df_encoded.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7dea2-f14e-45a8-89a3-2d0a67922a1e",
   "metadata": {},
   "source": [
    "## output Explanation:\n",
    "The code reads a dataset from a CSV file, performs one-hot encoding specifically on the 'Label' column ('sarcastic' vs 'not sarcastic'), and outputs a DataFrame (df_encoded) where each tweet's label is represented as a binary indicator column (Label_not sarcastic and Label_sarcastic). This transformation is useful for preparing categorical data for machine learning models that require numerical inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2a9d7-f60f-456c-8d52-150035d88d03",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f91a304b-1223-4911-8146-8c97910ecb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label                                             Tweets  \\\n",
      "0      1     I loovee when people text back  unamused_face    \n",
      "1      1  Don't you love it when your parents are Pissed...   \n",
      "2      1      So many useless classes , great to be student   \n",
      "3      1  Oh how I love getting home from work at am and...   \n",
      "4      1  I just love having grungy ass hair expressionl...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  ['i', 'lo', '##ove', '##e', 'when', 'people', ...   \n",
      "1  ['don', \"'\", 't', 'you', 'love', 'it', 'when',...   \n",
      "2  ['so', 'many', 'useless', 'classes', ',', 'gre...   \n",
      "3  ['oh', 'how', 'i', 'love', 'getting', 'home', ...   \n",
      "4  ['i', 'just', 'love', 'having', 'gr', '##ung',...   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  ['I', ' ', 'l', 'o', 'o', 'v', 'e', 'e', ' ', ...   \n",
      "1  ['D', 'o', 'n', \"'\", 't', ' ', 'y', 'o', 'u', ...   \n",
      "2  ['S', 'o', ' ', 'm', 'a', 'n', 'y', ' ', 'u', ...   \n",
      "3  ['O', 'h', ' ', 'h', 'o', 'w', ' ', 'I', ' ', ...   \n",
      "4  ['I', ' ', 'j', 'u', 's', 't', ' ', 'l', 'o', ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0  ['I loovee when people text back  unamused_face']   \n",
      "1  [\"Don't you love it when your parents are Piss...   \n",
      "2  ['So many useless classes , great to be student']   \n",
      "3  ['Oh how I love getting home from work at am a...   \n",
      "4  ['I just love having grungy ass hair expressio...   \n",
      "\n",
      "                                      subword_tokens  \\\n",
      "0  ['i', 'lo', '##ove', '##e', 'when', 'people', ...   \n",
      "1  ['don', \"'\", 't', 'you', 'love', 'it', 'when',...   \n",
      "2  ['so', 'many', 'useless', 'classes', ',', 'gre...   \n",
      "3  ['oh', 'how', 'i', 'love', 'getting', 'home', ...   \n",
      "4  ['i', 'just', 'love', 'having', 'gr', '##ung',...   \n",
      "\n",
      "                                    lemmatized_tweet  \n",
      "0       I loovee when people text back unamused_face  \n",
      "1  Do n't you love it when your parent are Pissed...  \n",
      "2        So many useless class , great to be student  \n",
      "3  Oh how I love getting home from work at am and...  \n",
      "4  I just love having grungy as hair expressionle...  \n",
      "      Label                                             Tweets  \\\n",
      "1970      0  Sometimes truth is glaring you in the face  bl...   \n",
      "1971      0    I just love not hanging out with my boyfriend .   \n",
      "1972      0  There is this 1 quince picture I have that I'm...   \n",
      "1973      0  I feel so ill at the moment that I cant speak ...   \n",
      "1974      0         I'm So Erked Nobody Better Not Speak To Me   \n",
      "\n",
      "                                            word_tokens  \\\n",
      "1970  ['sometimes', 'truth', 'is', 'glaring', 'you',...   \n",
      "1971  ['i', 'just', 'love', 'not', 'hanging', 'out',...   \n",
      "1972  ['there', 'is', 'this', '1', 'qui', '##nce', '...   \n",
      "1973  ['i', 'feel', 'so', 'ill', 'at', 'the', 'momen...   \n",
      "1974  ['i', \"'\", 'm', 'so', 'er', '##ked', 'nobody',...   \n",
      "\n",
      "                                            char_tokens  \\\n",
      "1970  ['S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', ...   \n",
      "1971  ['I', ' ', 'j', 'u', 's', 't', ' ', 'l', 'o', ...   \n",
      "1972  ['T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', ...   \n",
      "1973  ['I', ' ', 'f', 'e', 'e', 'l', ' ', 's', 'o', ...   \n",
      "1974  ['I', \"'\", 'm', ' ', 'S', 'o', ' ', 'E', 'r', ...   \n",
      "\n",
      "                                        sentence_tokens  \\\n",
      "1970  [\"Sometimes truth is glaring you in the face  ...   \n",
      "1971  ['I just love not hanging out with my boyfrien...   \n",
      "1972  [\"There is this 1 quince picture I have that I...   \n",
      "1973  [\"I feel so ill at the moment that I cant spea...   \n",
      "1974     [\"I'm So Erked Nobody Better Not Speak To Me\"]   \n",
      "\n",
      "                                         subword_tokens  \\\n",
      "1970  ['sometimes', 'truth', 'is', 'glaring', 'you',...   \n",
      "1971  ['i', 'just', 'love', 'not', 'hanging', 'out',...   \n",
      "1972  ['there', 'is', 'this', '1', 'qui', '##nce', '...   \n",
      "1973  ['i', 'feel', 'so', 'ill', 'at', 'the', 'momen...   \n",
      "1974  ['i', \"'\", 'm', 'so', 'er', '##ked', 'nobody',...   \n",
      "\n",
      "                                       lemmatized_tweet  \n",
      "1970  Sometimes truth is glaring you in the face bli...  \n",
      "1971    I just love not hanging out with my boyfriend .  \n",
      "1972  There is this 1 quince picture I have that I '...  \n",
      "1973  I feel so ill at the moment that I cant speak ...  \n",
      "1974        I 'm So Erked Nobody Better Not Speak To Me  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('tokenized_dataset.csv')\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "print(df.head())\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda02321-d540-4568-b7ba-6f5d2987b7d3",
   "metadata": {},
   "source": [
    "## output Explanation:\n",
    "\n",
    "Here, label_encoder = LabelEncoder() initializes an instance of LabelEncoder.\n",
    "\n",
    "df['Label'] = label_encoder.fit_transform(df['Label']) applies label encoding to the 'Label' column in the DataFrame df. The fit_transform method both fits the encoder to the unique categories in 'Label' and transforms them into integer labels.\n",
    "\n",
    "The output DataFrame will have the 'Label' column replaced with encoded integer labels. Each unique category in the original 'Label' column (e.g., 'sarcastic') will be replaced with a corresponding integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148fa9e-a70e-4f8a-995a-e1ddb7981444",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9700e412-da60-4343-aa76-c9911a34ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   045   06   10  100  1010  105  10k  10x   11  1179  ...  youur  yrs   yu  \\\n",
      "0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "5  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "6  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "7  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "8  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "9  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0  0.0   \n",
      "\n",
      "   yuh  yukno  zen  zero  zombie   zt  zzz  \n",
      "0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "2  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "3  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "4  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "5  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "6  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "7  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "8  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "9  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "\n",
      "[10 rows x 4152 columns]\n",
      "      045   06   10  100  1010  105  10k  10x   11  1179  ...  youur  yrs  \\\n",
      "1965  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1966  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1967  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1968  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1969  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1970  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1971  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1972  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1973  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "1974  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...    0.0  0.0   \n",
      "\n",
      "       yu  yuh  yukno  zen  zero  zombie   zt  zzz  \n",
      "1965  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1966  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1967  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1968  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1969  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1970  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1971  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1972  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1973  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "1974  0.0  0.0    0.0  0.0   0.0     0.0  0.0  0.0  \n",
      "\n",
      "[10 rows x 4152 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('tokenized_dataset.csv')\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Tweets'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e95260-9ddf-46ba-b0f2-a0082edc471b",
   "metadata": {},
   "source": [
    "## output Explanation:\n",
    "\n",
    "Here, tfidf_vectorizer = TfidfVectorizer() initializes an instance of TfidfVectorizer.\n",
    "\n",
    " tfidf_matrix = tfidf_vectorizer.fit_transform(df['Tweets']) fits the vectorizer on the 'Tweets' column in the DataFrame df and transforms it into a TF-IDF matrix.\n",
    " \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()) converts the TF-IDF matrix into a pandas DataFrame (tfidf_df) for easier inspection or further analysis.\n",
    "\n",
    "The output DataFrame (tfidf_df) will contain the TF-IDF representation of the 'Tweets' column. Each column represents a unique term (word) from the entire corpus, and each row corresponds to a document (tweet) with TF-IDF values indicating the importance of each term in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257573b5-4ea6-4146-b145-22767e6c5ac4",
   "metadata": {},
   "source": [
    "## Term Frequency Encoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc24e38b-de77-4047-894c-b46569631501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label  045  06  10  100  1010  105  10k  10x  11  ...  youur  yrs  yu  \\\n",
      "0  sarcastic    0   0   0    0     0    0    0    0   0  ...      0    0   0   \n",
      "1  sarcastic    0   0   0    0     0    0    0    0   0  ...      0    0   0   \n",
      "2  sarcastic    0   0   0    0     0    0    0    0   0  ...      0    0   0   \n",
      "3  sarcastic    0   0   0    0     0    0    0    0   0  ...      0    0   0   \n",
      "4  sarcastic    0   0   0    0     0    0    0    0   0  ...      0    0   0   \n",
      "\n",
      "   yuh  yukno  zen  zero  zombie  zt  zzz  \n",
      "0    0      0    0     0       0   0    0  \n",
      "1    0      0    0     0       0   0    0  \n",
      "2    0      0    0     0       0   0    0  \n",
      "3    0      0    0     0       0   0    0  \n",
      "4    0      0    0     0       0   0    0  \n",
      "\n",
      "[5 rows x 4153 columns]\n",
      "              Label  045  06  10  100  1010  105  10k  10x  11  ...  youur  \\\n",
      "1970  not sarcastic    0   0   0    0     0    0    0    0   0  ...      0   \n",
      "1971  not sarcastic    0   0   0    0     0    0    0    0   0  ...      0   \n",
      "1972  not sarcastic    0   0   0    0     0    0    0    0   0  ...      0   \n",
      "1973  not sarcastic    0   0   0    0     0    0    0    0   0  ...      0   \n",
      "1974  not sarcastic    0   0   0    0     0    0    0    0   0  ...      0   \n",
      "\n",
      "      yrs  yu  yuh  yukno  zen  zero  zombie  zt  zzz  \n",
      "1970    0   0    0      0    0     0       0   0    0  \n",
      "1971    0   0    0      0    0     0       0   0    0  \n",
      "1972    0   0    0      0    0     0       0   0    0  \n",
      "1973    0   0    0      0    0     0       0   0    0  \n",
      "1974    0   0    0      0    0     0       0   0    0  \n",
      "\n",
      "[5 rows x 4153 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv('tokenized_dataset.csv')\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform 'Tweets' column\n",
    "tf_matrix = count_vectorizer.fit_transform(df['Tweets'])\n",
    "\n",
    "# Convert the sparse matrix to a DataFrame \n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "tf_encoded_df = pd.concat([df['Label'], tf_df], axis=1)\n",
    "\n",
    "print(tf_encoded_df.head())\n",
    "print(tf_encoded_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf234e-93c1-48cb-a86d-ec905a5b8ef3",
   "metadata": {},
   "source": [
    "## output Explanation:\n",
    "\n",
    "count_vectorizer = CountVectorizer() initializes an instance of CountVectorizer, which will convert a collection of text documents to a matrix of term counts.\n",
    "\n",
    "tf_matrix = count_vectorizer.fit_transform(df['Tweets']) fits the vectorizer on the 'Tweets' column in the DataFrame df and transforms it into a term frequency matrix (tf_matrix).\n",
    "\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(), columns=count_vectorizer.get_feature_names_out()) converts the sparse term frequency matrix (tf_matrix) into a pandas DataFrame (tf_df) for easier inspection or further analysis.\n",
    "\n",
    "The output DataFrame (tf_encoded_df) will have the 'Label' column along with columns representing each term in the vocabulary extracted from the 'Tweets' column, where each cell contains the count of the corresponding term in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365d4be-ee9f-4356-a9ee-680df06cd508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
