{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263065fd-c393-4d4e-aea7-5d996052d0c7",
   "metadata": {},
   "source": [
    "# Different Tokenizations Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e6dfd-02bd-4757-8ef1-3a0bb42541e3",
   "metadata": {},
   "source": [
    " Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, subwords, characters, or even entire sentences, depending on the granularity required for a particular task or model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a6897-f51a-4cfa-a52a-ced84d7a5833",
   "metadata": {},
   "source": [
    "### Word Tokenization: \n",
    "Useful for most NLP tasks where words are treated as basic units, such as text classification or sentiment analysis. Word tokenization breaks text into individual words or terms.\n",
    "### Sentence Tokenization: \n",
    "Essential for tasks requiring analysis at the sentence level, such as machine translation or text summarization. Sentence tokenization splits text into individual sentences.\n",
    "### Character Tokenization: \n",
    "Useful for tasks where character-level analysis is important, such as handwriting recognition or certain types of text generation. Character tokenization breaks down text into individual characters.\n",
    "### Subword Tokenization: \n",
    "Particularly beneficial for handling unknown words or morphologically rich languages in NLP tasks like machine translation or named entity recognition. Subword tokenization divides text into smaller meaningful units or subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed51d3f7-ff50-444c-ba88-9268b99dfdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fde9dd28-3636-4459-8478-711130cbf6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>I loovee when people text back  unamused_face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>Don't you love it when your parents are Pissed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>So many useless classes , great to be student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>Oh how I love getting home from work at am and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sarcastic</td>\n",
       "      <td>I just love having grungy ass hair expressionl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                             Tweets\n",
       "0  sarcastic     I loovee when people text back  unamused_face \n",
       "1  sarcastic  Don't you love it when your parents are Pissed...\n",
       "2  sarcastic      So many useless classes , great to be student\n",
       "3  sarcastic  Oh how I love getting home from work at am and...\n",
       "4  sarcastic  I just love having grungy ass hair expressionl..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546ad44-cf13-4b0a-bd7e-2379779ffee5",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3897f276-1b1b-4241-ad41-dbe28e9688ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Label                                             Tweets  \\\n",
      "0   sarcastic     I loovee when people text back  unamused_face    \n",
      "1   sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2   sarcastic      So many useless classes , great to be student   \n",
      "3   sarcastic  Oh how I love getting home from work at am and...   \n",
      "4   sarcastic  I just love having grungy ass hair expressionl...   \n",
      "5   sarcastic  Thank you , random guy , for sneaking up behin...   \n",
      "6   sarcastic  Being half spanish and not being able to speak...   \n",
      "7   sarcastic  I know no one will remember it broken_heart   ...   \n",
      "8   sarcastic  Anyone in Chem that has to do OWLs knows my pa...   \n",
      "9   sarcastic                          Holy crap I look great      \n",
      "10  sarcastic      I love doing 20 sprints in 10 degree weather    \n",
      "11  sarcastic  feeling like a million bucks after that chem 2...   \n",
      "12  sarcastic  I love working in Sydney river It makes me wan...   \n",
      "13  sarcastic  I love hearing things about me that I didn't k...   \n",
      "14  sarcastic  I'm gonna let it slide today , but next time t...   \n",
      "\n",
      "                                               tokens  \n",
      "0   [I, loovee, when, people, text, back, unamused...  \n",
      "1   [Don't, you, love, it, when, your, parents, ar...  \n",
      "2   [So, many, useless, classes, ,, great, to, be,...  \n",
      "3   [Oh, how, I, love, getting, home, from, work, ...  \n",
      "4   [I, just, love, having, grungy, ass, hair, exp...  \n",
      "5   [Thank, you, ,, random, guy, ,, for, sneaking,...  \n",
      "6   [Being, half, spanish, and, not, being, able, ...  \n",
      "7   [I, know, no, one, will, remember, it, broken_...  \n",
      "8   [Anyone, in, Chem, that, has, to, do, OWLs, kn...  \n",
      "9                        [Holy, crap, I, look, great]  \n",
      "10  [I, love, doing, 20, sprints, in, 10, degree, ...  \n",
      "11  [feeling, like, a, million, bucks, after, that...  \n",
      "12  [I, love, working, in, Sydney, river, It, make...  \n",
      "13  [I, love, hearing, things, about, me, that, I,...  \n",
      "14  [I'm, gonna, let, it, slide, today, ,, but, ne...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Initialize the TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(Tweets):\n",
    "    return tokenizer.tokenize(Tweets)\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "df['tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "print(df.head(15))\n",
    "df.to_csv('tokenized_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab77d6-48ab-4f58-a42f-5c14a6da1b43",
   "metadata": {},
   "source": [
    "## Sentence Tokennization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b07b1ef-c42d-45eb-ac54-a92544fdd321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Label                                             Tweets  \\\n",
      "0   sarcastic     I loovee when people text back  unamused_face    \n",
      "1   sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2   sarcastic      So many useless classes , great to be student   \n",
      "3   sarcastic  Oh how I love getting home from work at am and...   \n",
      "4   sarcastic  I just love having grungy ass hair expressionl...   \n",
      "5   sarcastic  Thank you , random guy , for sneaking up behin...   \n",
      "6   sarcastic  Being half spanish and not being able to speak...   \n",
      "7   sarcastic  I know no one will remember it broken_heart   ...   \n",
      "8   sarcastic  Anyone in Chem that has to do OWLs knows my pa...   \n",
      "9   sarcastic                          Holy crap I look great      \n",
      "10  sarcastic      I love doing 20 sprints in 10 degree weather    \n",
      "11  sarcastic  feeling like a million bucks after that chem 2...   \n",
      "12  sarcastic  I love working in Sydney river It makes me wan...   \n",
      "13  sarcastic  I love hearing things about me that I didn't k...   \n",
      "14  sarcastic  I'm gonna let it slide today , but next time t...   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [I, loovee, when, people, text, back, unamused...   \n",
      "1   [Don't, you, love, it, when, your, parents, ar...   \n",
      "2   [So, many, useless, classes, ,, great, to, be,...   \n",
      "3   [Oh, how, I, love, getting, home, from, work, ...   \n",
      "4   [I, just, love, having, grungy, ass, hair, exp...   \n",
      "5   [Thank, you, ,, random, guy, ,, for, sneaking,...   \n",
      "6   [Being, half, spanish, and, not, being, able, ...   \n",
      "7   [I, know, no, one, will, remember, it, broken_...   \n",
      "8   [Anyone, in, Chem, that, has, to, do, OWLs, kn...   \n",
      "9                        [Holy, crap, I, look, great]   \n",
      "10  [I, love, doing, 20, sprints, in, 10, degree, ...   \n",
      "11  [feeling, like, a, million, bucks, after, that...   \n",
      "12  [I, love, working, in, Sydney, river, It, make...   \n",
      "13  [I, love, hearing, things, about, me, that, I,...   \n",
      "14  [I'm, gonna, let, it, slide, today, ,, but, ne...   \n",
      "\n",
      "                                      sentence_tokens  \n",
      "0     [I loovee when people text back  unamused_face]  \n",
      "1   [Don't you love it when your parents are Pisse...  \n",
      "2     [So many useless classes , great to be student]  \n",
      "3   [Oh how I love getting home from work at am an...  \n",
      "4   [I just love having grungy ass hair expression...  \n",
      "5   [Thank you , random guy , for sneaking up behi...  \n",
      "6   [Being half spanish and not being able to spea...  \n",
      "7       [I know no one will remember it broken_heart]  \n",
      "8   [Anyone in Chem that has to do OWLs knows my p...  \n",
      "9                            [Holy crap I look great]  \n",
      "10     [I love doing 20 sprints in 10 degree weather]  \n",
      "11  [feeling like a million bucks after that chem ...  \n",
      "12  [I love working in Sydney river It makes me wa...  \n",
      "13  [I love hearing things about me that I didn't ...  \n",
      "14  [I'm gonna let it slide today , but next time ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "\n",
    "# Initialize the TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "    \n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "# Tokenize tweets into words\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "# Tokenize tweets into sentences\n",
    "df['sentence_tokens'] = df['Tweets'].apply(tokenize_sentences)\n",
    "print(df.head(15))\n",
    "df.to_csv('tokenized_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30595a86-8ae6-4c10-83b2-af964638d2fe",
   "metadata": {},
   "source": [
    "## Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4acfc23d-ee42-43c5-bfe3-2ac11c28ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Label                                             Tweets  \\\n",
      "0   sarcastic     I loovee when people text back  unamused_face    \n",
      "1   sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2   sarcastic      So many useless classes , great to be student   \n",
      "3   sarcastic  Oh how I love getting home from work at am and...   \n",
      "4   sarcastic  I just love having grungy ass hair expressionl...   \n",
      "5   sarcastic  Thank you , random guy , for sneaking up behin...   \n",
      "6   sarcastic  Being half spanish and not being able to speak...   \n",
      "7   sarcastic  I know no one will remember it broken_heart   ...   \n",
      "8   sarcastic  Anyone in Chem that has to do OWLs knows my pa...   \n",
      "9   sarcastic                          Holy crap I look great      \n",
      "10  sarcastic      I love doing 20 sprints in 10 degree weather    \n",
      "11  sarcastic  feeling like a million bucks after that chem 2...   \n",
      "12  sarcastic  I love working in Sydney river It makes me wan...   \n",
      "13  sarcastic  I love hearing things about me that I didn't k...   \n",
      "14  sarcastic  I'm gonna let it slide today , but next time t...   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [I, loovee, when, people, text, back, unamused...   \n",
      "1   [Don't, you, love, it, when, your, parents, ar...   \n",
      "2   [So, many, useless, classes, ,, great, to, be,...   \n",
      "3   [Oh, how, I, love, getting, home, from, work, ...   \n",
      "4   [I, just, love, having, grungy, ass, hair, exp...   \n",
      "5   [Thank, you, ,, random, guy, ,, for, sneaking,...   \n",
      "6   [Being, half, spanish, and, not, being, able, ...   \n",
      "7   [I, know, no, one, will, remember, it, broken_...   \n",
      "8   [Anyone, in, Chem, that, has, to, do, OWLs, kn...   \n",
      "9                        [Holy, crap, I, look, great]   \n",
      "10  [I, love, doing, 20, sprints, in, 10, degree, ...   \n",
      "11  [feeling, like, a, million, bucks, after, that...   \n",
      "12  [I, love, working, in, Sydney, river, It, make...   \n",
      "13  [I, love, hearing, things, about, me, that, I,...   \n",
      "14  [I'm, gonna, let, it, slide, today, ,, but, ne...   \n",
      "\n",
      "                                      sentence_tokens  \\\n",
      "0     [I loovee when people text back  unamused_face]   \n",
      "1   [Don't you love it when your parents are Pisse...   \n",
      "2     [So many useless classes , great to be student]   \n",
      "3   [Oh how I love getting home from work at am an...   \n",
      "4   [I just love having grungy ass hair expression...   \n",
      "5   [Thank you , random guy , for sneaking up behi...   \n",
      "6   [Being half spanish and not being able to spea...   \n",
      "7       [I know no one will remember it broken_heart]   \n",
      "8   [Anyone in Chem that has to do OWLs knows my p...   \n",
      "9                            [Holy crap I look great]   \n",
      "10     [I love doing 20 sprints in 10 degree weather]   \n",
      "11  [feeling like a million bucks after that chem ...   \n",
      "12  [I love working in Sydney river It makes me wa...   \n",
      "13  [I love hearing things about me that I didn't ...   \n",
      "14  [I'm gonna let it slide today , but next time ...   \n",
      "\n",
      "                                          char_tokens  \n",
      "0   [I,  , l, o, o, v, e, e,  , w, h, e, n,  , p, ...  \n",
      "1   [D, o, n, ', t,  , y, o, u,  , l, o, v, e,  , ...  \n",
      "2   [S, o,  , m, a, n, y,  , u, s, e, l, e, s, s, ...  \n",
      "3   [O, h,  , h, o, w,  , I,  , l, o, v, e,  , g, ...  \n",
      "4   [I,  , j, u, s, t,  , l, o, v, e,  , h, a, v, ...  \n",
      "5   [T, h, a, n, k,  , y, o, u,  , ,,  , r, a, n, ...  \n",
      "6   [B, e, i, n, g,  , h, a, l, f,  , s, p, a, n, ...  \n",
      "7   [I,  , k, n, o, w,  , n, o,  , o, n, e,  , w, ...  \n",
      "8   [A, n, y, o, n, e,  , i, n,  , C, h, e, m,  , ...  \n",
      "9   [H, o, l, y,  , c, r, a, p,  , I,  , l, o, o, ...  \n",
      "10  [I,  , l, o, v, e,  , d, o, i, n, g,  , 2, 0, ...  \n",
      "11  [f, e, e, l, i, n, g,  , l, i, k, e,  , a,  , ...  \n",
      "12  [I,  , l, o, v, e,  , w, o, r, k, i, n, g,  , ...  \n",
      "13  [I,  , l, o, v, e,  , h, e, a, r, i, n, g,  , ...  \n",
      "14  [I, ', m,  , g, o, n, n, a,  , l, e, t,  , i, ...  \n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize text into words using TweetTokenizer\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "# sent_tokenize\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# characters\n",
    "def tokenize_characters(text):\n",
    "    return list(text) \n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweets'].apply(tokenize_sentences)\n",
    "\n",
    "df['char_tokens'] = df['Tweets'].apply(tokenize_characters)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\tokenized_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aec816-2ebc-4a25-a6ed-20ab5d4deccb",
   "metadata": {},
   "source": [
    "## Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8e20600-7524-4aea-a290-3b78c99105a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Label                                             Tweets  \\\n",
      "0   sarcastic     I loovee when people text back  unamused_face    \n",
      "1   sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2   sarcastic      So many useless classes , great to be student   \n",
      "3   sarcastic  Oh how I love getting home from work at am and...   \n",
      "4   sarcastic  I just love having grungy ass hair expressionl...   \n",
      "5   sarcastic  Thank you , random guy , for sneaking up behin...   \n",
      "6   sarcastic  Being half spanish and not being able to speak...   \n",
      "7   sarcastic  I know no one will remember it broken_heart   ...   \n",
      "8   sarcastic  Anyone in Chem that has to do OWLs knows my pa...   \n",
      "9   sarcastic                          Holy crap I look great      \n",
      "10  sarcastic      I love doing 20 sprints in 10 degree weather    \n",
      "11  sarcastic  feeling like a million bucks after that chem 2...   \n",
      "12  sarcastic  I love working in Sydney river It makes me wan...   \n",
      "13  sarcastic  I love hearing things about me that I didn't k...   \n",
      "14  sarcastic  I'm gonna let it slide today , but next time t...   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [i, lo, ##ove, ##e, when, people, text, back, ...   \n",
      "1   [don, ', t, you, love, it, when, your, parents...   \n",
      "2   [so, many, useless, classes, ,, great, to, be,...   \n",
      "3   [oh, how, i, love, getting, home, from, work, ...   \n",
      "4   [i, just, love, having, gr, ##ung, ##y, ass, h...   \n",
      "5   [thank, you, ,, random, guy, ,, for, sneaking,...   \n",
      "6   [being, half, spanish, and, not, being, able, ...   \n",
      "7   [i, know, no, one, will, remember, it, broken,...   \n",
      "8   [anyone, in, che, ##m, that, has, to, do, owls...   \n",
      "9                        [holy, crap, i, look, great]   \n",
      "10  [i, love, doing, 20, sprint, ##s, in, 10, degr...   \n",
      "11  [feeling, like, a, million, bucks, after, that...   \n",
      "12  [i, love, working, in, sydney, river, it, make...   \n",
      "13  [i, love, hearing, things, about, me, that, i,...   \n",
      "14  [i, ', m, gonna, let, it, slide, today, ,, but...   \n",
      "\n",
      "                                          char_tokens  \\\n",
      "0   [I,  , l, o, o, v, e, e,  , w, h, e, n,  , p, ...   \n",
      "1   [D, o, n, ', t,  , y, o, u,  , l, o, v, e,  , ...   \n",
      "2   [S, o,  , m, a, n, y,  , u, s, e, l, e, s, s, ...   \n",
      "3   [O, h,  , h, o, w,  , I,  , l, o, v, e,  , g, ...   \n",
      "4   [I,  , j, u, s, t,  , l, o, v, e,  , h, a, v, ...   \n",
      "5   [T, h, a, n, k,  , y, o, u,  , ,,  , r, a, n, ...   \n",
      "6   [B, e, i, n, g,  , h, a, l, f,  , s, p, a, n, ...   \n",
      "7   [I,  , k, n, o, w,  , n, o,  , o, n, e,  , w, ...   \n",
      "8   [A, n, y, o, n, e,  , i, n,  , C, h, e, m,  , ...   \n",
      "9   [H, o, l, y,  , c, r, a, p,  , I,  , l, o, o, ...   \n",
      "10  [I,  , l, o, v, e,  , d, o, i, n, g,  , 2, 0, ...   \n",
      "11  [f, e, e, l, i, n, g,  , l, i, k, e,  , a,  , ...   \n",
      "12  [I,  , l, o, v, e,  , w, o, r, k, i, n, g,  , ...   \n",
      "13  [I,  , l, o, v, e,  , h, e, a, r, i, n, g,  , ...   \n",
      "14  [I, ', m,  , g, o, n, n, a,  , l, e, t,  , i, ...   \n",
      "\n",
      "                                      sentence_tokens  \\\n",
      "0     [I loovee when people text back  unamused_face]   \n",
      "1   [Don't you love it when your parents are Pisse...   \n",
      "2     [So many useless classes , great to be student]   \n",
      "3   [Oh how I love getting home from work at am an...   \n",
      "4   [I just love having grungy ass hair expression...   \n",
      "5   [Thank you , random guy , for sneaking up behi...   \n",
      "6   [Being half spanish and not being able to spea...   \n",
      "7       [I know no one will remember it broken_heart]   \n",
      "8   [Anyone in Chem that has to do OWLs knows my p...   \n",
      "9                            [Holy crap I look great]   \n",
      "10     [I love doing 20 sprints in 10 degree weather]   \n",
      "11  [feeling like a million bucks after that chem ...   \n",
      "12  [I love working in Sydney river It makes me wa...   \n",
      "13  [I love hearing things about me that I didn't ...   \n",
      "14  [I'm gonna let it slide today , but next time ...   \n",
      "\n",
      "                                       subword_tokens  \n",
      "0   [i, lo, ##ove, ##e, when, people, text, back, ...  \n",
      "1   [don, ', t, you, love, it, when, your, parents...  \n",
      "2   [so, many, useless, classes, ,, great, to, be,...  \n",
      "3   [oh, how, i, love, getting, home, from, work, ...  \n",
      "4   [i, just, love, having, gr, ##ung, ##y, ass, h...  \n",
      "5   [thank, you, ,, random, guy, ,, for, sneaking,...  \n",
      "6   [being, half, spanish, and, not, being, able, ...  \n",
      "7   [i, know, no, one, will, remember, it, broken,...  \n",
      "8   [anyone, in, che, ##m, that, has, to, do, owls...  \n",
      "9                        [holy, crap, i, look, great]  \n",
      "10  [i, love, doing, 20, sprint, ##s, in, 10, degr...  \n",
      "11  [feeling, like, a, million, bucks, after, that...  \n",
      "12  [i, love, working, in, sydney, river, it, make...  \n",
      "13  [i, love, hearing, things, about, me, that, i,...  \n",
      "14  [i, ', m, gonna, let, it, slide, today, ,, but...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_subwords(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweets'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweets'].apply(tokenize_sentences)\n",
    "\n",
    "df['subword_tokens'] = df['Tweets'].apply(tokenize_subwords)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('tokenized_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb4e18-d7ea-40d5-95ef-1402e7b1aee9",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999fd01-2209-49d4-ac8e-ae69f0087ac7",
   "metadata": {},
   "source": [
    " Lemmatization is a process in natural language processing (NLP) that involves reducing words to their base or root form, known as the lemma. The goal of lemmatization is to normalize words so that different forms of the same word are treated as the same token. \n",
    " \n",
    "Examples:\n",
    "\n",
    "Lemmatization of the word \"running\":\n",
    "\n",
    "Stemming: \"running\" -> \"run\"\n",
    "\n",
    "Lemmatization: \"running\" -> \"run\"\n",
    "\n",
    "In NLP tasks such as text classification, sentiment analysis, and information retrieval, lemmatization helps to reduce the complexity of text data and improve the accuracy of models by normalizing words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84aeff9f-5e18-4ea0-be15-ec8855b04397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label                                             Tweets  \\\n",
      "0  sarcastic     I loovee when people text back  unamused_face    \n",
      "1  sarcastic  Don't you love it when your parents are Pissed...   \n",
      "2  sarcastic      So many useless classes , great to be student   \n",
      "3  sarcastic  Oh how I love getting home from work at am and...   \n",
      "4  sarcastic  I just love having grungy ass hair expressionl...   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  [I, loovee, when, people, text, back, unamused...   \n",
      "1  [Don't, you, love, it, when, your, parents, ar...   \n",
      "2  [So, many, useless, classes, ,, great, to, be,...   \n",
      "3  [Oh, how, I, love, getting, home, from, work, ...   \n",
      "4  [I, just, love, having, grungy, ass, hair, exp...   \n",
      "\n",
      "                                   lemmatized_tokens  \n",
      "0  [I, loovee, when, people, text, back, unamused...  \n",
      "1  [Don't, you, love, it, when, your, parent, be,...  \n",
      "2  [So, many, useless, class, ,, great, to, be, s...  \n",
      "3  [Oh, how, I, love, get, home, from, work, at, ...  \n",
      "4  [I, just, love, have, grungy, as, hair, expres...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize words\n",
    "def lemmatize_words(tokens):\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag([token])[0][1][0].upper() \n",
    "        pos_tag = pos_tag if pos_tag in ['A', 'N', 'V'] else 'n' \n",
    "        lemma_tokens.append(lemmatizer.lemmatize(token, pos=pos_tag.lower()))\n",
    "    \n",
    "    return lemma_tokens\n",
    "\n",
    "df = pd.read_csv('Clean_Dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweets'].apply(tokenizer.tokenize)\n",
    "\n",
    "df['lemmatized_tokens'] = df['word_tokens'].apply(lemmatize_words)\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "df.to_csv('lemmatized_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f72fa-9fc1-4f3d-888c-7bef51094cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
