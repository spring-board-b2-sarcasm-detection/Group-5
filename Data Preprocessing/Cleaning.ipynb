{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73b7139-ffc4-4efd-b894-fb607903bce6",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274da1d6-b2e3-4d56-8312-5ceca760399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7693f94-a793-4863-914f-ddc89a8cffc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic service yet again from EE. 1st you u...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure if that was or. I will take it! face_...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barely 9 am and already shaking with rage.</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I guess that proves it then. Black folks have ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does this tweet need a tag</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Sarcasm\n",
       "0  Fantastic service yet again from EE. 1st you u...     yes\n",
       "1  Not sure if that was or. I will take it! face_...     yes\n",
       "2         Barely 9 am and already shaking with rage.     yes\n",
       "3  I guess that proves it then. Black folks have ...     yes\n",
       "4                         Does this tweet need a tag     yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('merged_file.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea90b88-2552-4d22-ae29-8dd67f569fe7",
   "metadata": {},
   "source": [
    "# Different ways of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ec144-f5d4-432b-9050-e11cb7dcde1b",
   "metadata": {},
   "source": [
    "## 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcfed57-fb5f-4828-9315-953612fc5186",
   "metadata": {},
   "source": [
    "## 2. Character Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e899916-8fc7-41e7-90d2-2eaa74f1ca80",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfcff3-5d4f-4cdd-9480-1589ee9abac8",
   "metadata": {},
   "source": [
    "## 4. Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b157b4-ff54-461d-b13c-d6e3f6990cda",
   "metadata": {},
   "source": [
    "# Technique Number 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39763c4-0d40-4d4f-895b-ae7f1fb7214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                               tokens  \n",
      "0   [Fantastic, service, yet, again, from, EE, ., ...  \n",
      "1   [Not, sure, if, that, was, or, ., I, will, tak...  \n",
      "2   [Barely, 9, am, and, already, shaking, with, r...  \n",
      "3   [I, guess, that, proves, it, then, ., Black, f...  \n",
      "4                   [Does, this, tweet, need, a, tag]  \n",
      "5   [both, ., Wont, be, using, you, again, ., Made...  \n",
      "6      [Fuuuuuuuuck, this, shit, screams, thug, !, !]  \n",
      "7   [66, UEs, from, Staniel, and, he, wins, in, st...  \n",
      "8       [totally, surprising, to, every, husker, fan]  \n",
      "9              [Haha, got, to, love, the, enthusiasm]  \n",
      "10  [a, southern, pride, advocate, but, of, course...  \n",
      "11  [So, nice, not, hearing, any, rumors, that, th...  \n",
      "12  [We, should, divide, illegals, into, two, line...  \n",
      "13  [does, not, even, look, like, that, great, of,...  \n",
      "14  [Mondays, are, always, dreadful, ., Why, are, ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Initialize the TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(Tweet):\n",
    "    return tokenizer.tokenize(Tweet)\n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "df['tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "print(df.head(15))\n",
    "df.to_csv('t_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932c83f-0d3b-47d4-ae7f-083918ee4003",
   "metadata": {},
   "source": [
    "# Technique Number 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e7e200-c29c-484d-856c-41bee07e6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [Fantastic, service, yet, again, from, EE, ., ...   \n",
      "1   [Not, sure, if, that, was, or, ., I, will, tak...   \n",
      "2   [Barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [I, guess, that, proves, it, then, ., Black, f...   \n",
      "4                   [Does, this, tweet, need, a, tag]   \n",
      "5   [both, ., Wont, be, using, you, again, ., Made...   \n",
      "6      [Fuuuuuuuuck, this, shit, screams, thug, !, !]   \n",
      "7   [66, UEs, from, Staniel, and, he, wins, in, st...   \n",
      "8       [totally, surprising, to, every, husker, fan]   \n",
      "9              [Haha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [So, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [We, should, divide, illegals, into, two, line...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [Mondays, are, always, dreadful, ., Why, are, ...   \n",
      "\n",
      "                                          char_tokens  \n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...  \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...  \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...  \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...  \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...  \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...  \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...  \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...  \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...  \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...  \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...  \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...  \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...  \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...  \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def tokenize_characters(text):\n",
    "    return list(text)  \n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('t_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b441e5-8eef-45ae-bfbd-83cd88ef5b66",
   "metadata": {},
   "source": [
    "# Technique Number 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3af15755-308a-4139-93d5-39ece4c02f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jiyagoyal/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [Fantastic, service, yet, again, from, EE, ., ...   \n",
      "1   [Not, sure, if, that, was, or, ., I, will, tak...   \n",
      "2   [Barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [I, guess, that, proves, it, then, ., Black, f...   \n",
      "4                   [Does, this, tweet, need, a, tag]   \n",
      "5   [both, ., Wont, be, using, you, again, ., Made...   \n",
      "6      [Fuuuuuuuuck, this, shit, screams, thug, !, !]   \n",
      "7   [66, UEs, from, Staniel, and, he, wins, in, st...   \n",
      "8       [totally, surprising, to, every, husker, fan]   \n",
      "9              [Haha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [So, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [We, should, divide, illegals, into, two, line...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [Mondays, are, always, dreadful, ., Why, are, ...   \n",
      "\n",
      "                                          char_tokens  \\\n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...   \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...   \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...   \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...   \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...   \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...   \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...   \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...   \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...   \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...   \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...   \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...   \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...   \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...   \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...   \n",
      "\n",
      "                                      sentence_tokens  \n",
      "0   [Fantastic service yet again from EE., 1st you...  \n",
      "1   [Not sure if that was or., I will take it!, fa...  \n",
      "2        [Barely 9 am and already shaking with rage.]  \n",
      "3   [I guess that proves it then., Black folks hav...  \n",
      "4                        [Does this tweet need a tag]  \n",
      "5   [both., Wont be using you again., Made a forma...  \n",
      "6            [Fuuuuuuuuck this shit screams thug!, !]  \n",
      "7   [66 UEs from Staniel and he wins in straights....  \n",
      "8            [totally surprising to every husker fan]  \n",
      "9                   [Haha got to love the enthusiasm]  \n",
      "10  [a southern pride advocate but of course it ha...  \n",
      "11  [So nice not hearing any rumors that the  are ...  \n",
      "12  [We should divide illegals into two lines one ...  \n",
      "13    [does not even look like that great of a view.]  \n",
      "14  [Mondays are always dreadful., Why are Fridays...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "    \n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweet'].apply(tokenize_sentences)\n",
    "print(df.head(15))\n",
    "df.to_csv('t_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0deeb3c-6c75-46e7-8d32-4bde3ccbcb24",
   "metadata": {},
   "source": [
    "# Technique Number 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c6a08d0-42c6-4c49-b16b-bec9b170801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.3/410.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a187b4db-408a-4d52-8583-d34b71ac7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [fantastic, service, yet, again, from, ee, ., ...   \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...   \n",
      "2   [barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [i, guess, that, proves, it, then, ., black, f...   \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]   \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...   \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...   \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...   \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...   \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...   \n",
      "\n",
      "                                          char_tokens  \\\n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...   \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...   \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...   \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...   \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...   \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...   \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...   \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...   \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...   \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...   \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...   \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...   \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...   \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...   \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...   \n",
      "\n",
      "                                      sentence_tokens  \\\n",
      "0   [Fantastic service yet again from EE., 1st you...   \n",
      "1   [Not sure if that was or., I will take it!, fa...   \n",
      "2        [Barely 9 am and already shaking with rage.]   \n",
      "3   [I guess that proves it then., Black folks hav...   \n",
      "4                        [Does this tweet need a tag]   \n",
      "5   [both., Wont be using you again., Made a forma...   \n",
      "6            [Fuuuuuuuuck this shit screams thug!, !]   \n",
      "7   [66 UEs from Staniel and he wins in straights....   \n",
      "8            [totally surprising to every husker fan]   \n",
      "9                   [Haha got to love the enthusiasm]   \n",
      "10  [a southern pride advocate but of course it ha...   \n",
      "11  [So nice not hearing any rumors that the  are ...   \n",
      "12  [We should divide illegals into two lines one ...   \n",
      "13    [does not even look like that great of a view.]   \n",
      "14  [Mondays are always dreadful., Why are Fridays...   \n",
      "\n",
      "                                       subword_tokens  \n",
      "0   [fantastic, service, yet, again, from, ee, ., ...  \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...  \n",
      "2   [barely, 9, am, and, already, shaking, with, r...  \n",
      "3   [i, guess, that, proves, it, then, ., black, f...  \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]  \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...  \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...  \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...  \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...  \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]  \n",
      "10  [a, southern, pride, advocate, but, of, course...  \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...  \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...  \n",
      "13  [does, not, even, look, like, that, great, of,...  \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_subwords(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweet'].apply(tokenize_sentences)\n",
    "\n",
    "df['subword_tokens'] = df['Tweet'].apply(tokenize_subwords)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('t_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c9705-209d-441c-ac8c-40ea2a9cc0c1",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b1cba9f-6161-4a63-b894-a60b383cd0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jiyagoyal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jiyagoyal/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [fantastic, service, yet, again, from, ee, ., ...   \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...   \n",
      "2   [barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [i, guess, that, proves, it, then, ., black, f...   \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]   \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...   \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...   \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...   \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...   \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...   \n",
      "\n",
      "                                          char_tokens  \\\n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...   \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...   \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...   \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...   \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...   \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...   \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...   \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...   \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...   \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...   \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...   \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...   \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...   \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...   \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...   \n",
      "\n",
      "                                      sentence_tokens  \\\n",
      "0   [Fantastic service yet again from EE., 1st you...   \n",
      "1   [Not sure if that was or., I will take it!, fa...   \n",
      "2        [Barely 9 am and already shaking with rage.]   \n",
      "3   [I guess that proves it then., Black folks hav...   \n",
      "4                        [Does this tweet need a tag]   \n",
      "5   [both., Wont be using you again., Made a forma...   \n",
      "6            [Fuuuuuuuuck this shit screams thug!, !]   \n",
      "7   [66 UEs from Staniel and he wins in straights....   \n",
      "8            [totally surprising to every husker fan]   \n",
      "9                   [Haha got to love the enthusiasm]   \n",
      "10  [a southern pride advocate but of course it ha...   \n",
      "11  [So nice not hearing any rumors that the  are ...   \n",
      "12  [We should divide illegals into two lines one ...   \n",
      "13    [does not even look like that great of a view.]   \n",
      "14  [Mondays are always dreadful., Why are Fridays...   \n",
      "\n",
      "                                       subword_tokens  \\\n",
      "0   [fantastic, service, yet, again, from, ee, ., ...   \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...   \n",
      "2   [barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [i, guess, that, proves, it, then, ., black, f...   \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]   \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...   \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...   \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...   \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...   \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...   \n",
      "\n",
      "                                     lemmatized_tweet  \n",
      "0   Fantastic service yet again from EE . 1st you ...  \n",
      "1   Not sure if that wa or . I will take it ! face...  \n",
      "2         Barely 9 am and already shaking with rage .  \n",
      "3   I guess that prof it then . Black folk have no...  \n",
      "4                          Does this tweet need a tag  \n",
      "5   both . Wont be using you again . Made a formal...  \n",
      "6               Fuuuuuuuuck this shit scream thug ! !  \n",
      "7   66 UEs from Staniel and he win in straight . B...  \n",
      "8              totally surprising to every husker fan  \n",
      "9                     Haha got to love the enthusiasm  \n",
      "10  a southern pride advocate but of course it ha ...  \n",
      "11  So nice not hearing any rumor that the are not...  \n",
      "12  We should divide illegals into two line one fo...  \n",
      "13      doe not even look like that great of a view .  \n",
      "14  Mondays are always dreadful . Why are Fridays ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweet'].apply(tokenize_sentences)\n",
    "\n",
    "df['subword_tokens'] = df['Tweet'].apply(tokenize_subwords)\n",
    "\n",
    "df['lemmatized_tweet'] = df['Tweet'].apply(lemmatize_text)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('t_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc170c6-7361-44df-b034-e37738fca2fe",
   "metadata": {},
   "source": [
    "# Encoding Techniques;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07600016-c010-41ac-9a3e-6ed20f9c3247",
   "metadata": {},
   "source": [
    "### One-Hot Encoding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11b86881-0472-4b7c-91b6-128cb87734e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "                                               Tweet Sarcasm  \\\n",
      "0  Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1  Not sure if that was or. I will take it! face_...     yes   \n",
      "2         Barely 9 am and already shaking with rage.     yes   \n",
      "3  I guess that proves it then. Black folks have ...     yes   \n",
      "4                         Does this tweet need a tag     yes   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  ['fantastic', 'service', 'yet', 'again', 'from...   \n",
      "1  ['not', 'sure', 'if', 'that', 'was', 'or', '.'...   \n",
      "2  ['barely', '9', 'am', 'and', 'already', 'shaki...   \n",
      "3  ['i', 'guess', 'that', 'proves', 'it', 'then',...   \n",
      "4  ['does', 'this', 't', '##wee', '##t', 'need', ...   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  ['F', 'a', 'n', 't', 'a', 's', 't', 'i', 'c', ...   \n",
      "1  ['N', 'o', 't', ' ', 's', 'u', 'r', 'e', ' ', ...   \n",
      "2  ['B', 'a', 'r', 'e', 'l', 'y', ' ', '9', ' ', ...   \n",
      "3  ['I', ' ', 'g', 'u', 'e', 's', 's', ' ', 't', ...   \n",
      "4  ['D', 'o', 'e', 's', ' ', 't', 'h', 'i', 's', ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0  ['Fantastic service yet again from EE.', '1st ...   \n",
      "1  ['Not sure if that was or.', 'I will take it!'...   \n",
      "2     ['Barely 9 am and already shaking with rage.']   \n",
      "3  ['I guess that proves it then.', 'Black folks ...   \n",
      "4                     ['Does this tweet need a tag']   \n",
      "\n",
      "                                      subword_tokens  \\\n",
      "0  ['fantastic', 'service', 'yet', 'again', 'from...   \n",
      "1  ['not', 'sure', 'if', 'that', 'was', 'or', '.'...   \n",
      "2  ['barely', '9', 'am', 'and', 'already', 'shaki...   \n",
      "3  ['i', 'guess', 'that', 'proves', 'it', 'then',...   \n",
      "4  ['does', 'this', 't', '##wee', '##t', 'need', ...   \n",
      "\n",
      "                                    lemmatized_tweet  \n",
      "0  Fantastic service yet again from EE . 1st you ...  \n",
      "1  Not sure if that wa or . I will take it ! face...  \n",
      "2        Barely 9 am and already shaking with rage .  \n",
      "3  I guess that prof it then . Black folk have no...  \n",
      "4                         Does this tweet need a tag  \n",
      "\n",
      "One-hot encoded DataFrame:\n",
      "                                               Tweet Sarcasm  \\\n",
      "0  Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1  Not sure if that was or. I will take it! face_...     yes   \n",
      "2         Barely 9 am and already shaking with rage.     yes   \n",
      "3  I guess that proves it then. Black folks have ...     yes   \n",
      "4                         Does this tweet need a tag     yes   \n",
      "\n",
      "                                         word_tokens  \\\n",
      "0  ['fantastic', 'service', 'yet', 'again', 'from...   \n",
      "1  ['not', 'sure', 'if', 'that', 'was', 'or', '.'...   \n",
      "2  ['barely', '9', 'am', 'and', 'already', 'shaki...   \n",
      "3  ['i', 'guess', 'that', 'proves', 'it', 'then',...   \n",
      "4  ['does', 'this', 't', '##wee', '##t', 'need', ...   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  ['F', 'a', 'n', 't', 'a', 's', 't', 'i', 'c', ...   \n",
      "1  ['N', 'o', 't', ' ', 's', 'u', 'r', 'e', ' ', ...   \n",
      "2  ['B', 'a', 'r', 'e', 'l', 'y', ' ', '9', ' ', ...   \n",
      "3  ['I', ' ', 'g', 'u', 'e', 's', 's', ' ', 't', ...   \n",
      "4  ['D', 'o', 'e', 's', ' ', 't', 'h', 'i', 's', ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0  ['Fantastic service yet again from EE.', '1st ...   \n",
      "1  ['Not sure if that was or.', 'I will take it!'...   \n",
      "2     ['Barely 9 am and already shaking with rage.']   \n",
      "3  ['I guess that proves it then.', 'Black folks ...   \n",
      "4                     ['Does this tweet need a tag']   \n",
      "\n",
      "                                      subword_tokens  \\\n",
      "0  ['fantastic', 'service', 'yet', 'again', 'from...   \n",
      "1  ['not', 'sure', 'if', 'that', 'was', 'or', '.'...   \n",
      "2  ['barely', '9', 'am', 'and', 'already', 'shaki...   \n",
      "3  ['i', 'guess', 'that', 'proves', 'it', 'then',...   \n",
      "4  ['does', 'this', 't', '##wee', '##t', 'need', ...   \n",
      "\n",
      "                                    lemmatized_tweet  \\\n",
      "0  Fantastic service yet again from EE . 1st you ...   \n",
      "1  Not sure if that wa or . I will take it ! face...   \n",
      "2        Barely 9 am and already shaking with rage .   \n",
      "3  I guess that prof it then . Black folk have no...   \n",
      "4                         Does this tweet need a tag   \n",
      "\n",
      "   Tweet_  How the drug war is bankrupting higher education thank you pot smokers  \\nTrainSen\\t0\\tSchool soon . Great . How I missed seeing your face .\\nTrainSen\\t0\\tChillin ' with a game of Rock Band  . Fun times to be had .\\nTrainSen\\t0\\tId give my right arm to be in the Paralympics\\nTrainSen\\t0\\tSo the protesters are planning on disrupting this council meeting too . They act as if when they ask to speak at \\nTrainSen\\t0\\tThis is easily one of my fav Cimorelliband videos  LaurenCimorelli LisaCim KathCim , you  outdid yourselves \\nTrainSen\\t0\\tI love fundraisers . Schools you can made so much off sports fundraising . Contact me I would live to speak with \\nTrainSen\\t0\\tWho you gonna call  Ghostbusters \\nTrainSen\\t0\\tHey at least theres a nice breeze to blow around the hot n humid air . \\nTrainSen\\t0\\tYour life seems to grow crazier today , as if everything hasn't  More for Aries\\nTrainSen\\t0\\tOur friends at the Fowler House in Quincy love Bawstins so much they may be adding it to their favorite dish . Look out its gonna be awesome \\nTrainSen\\t0\\t God will supply us w it h the opportunity , but it's up to us to do something with it .   \\\n",
      "0                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "1                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "2                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "3                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "4                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "\n",
      "   Tweet_  It was so great to see u again  Thanks 4 allowing me the opportunity 2 speak about gratitude  . My pleasure\\nTrainSen\\t0\\tI can write but I cannot speak . I know English but I prefer to speak in Malay . Homagoshh  I feel so stupid . Urghh\\nTrainSen\\t0\\tPeople say I'm so hard to speak to\\nTrainSen\\t0\\tI'd like to speak to someone at your national office , could you follow back so I can give u more info \\nTrainSen\\t0\\tI wonder what POTUS listens to when he's in the car enroute to speak . He's in ATL today so I imagine him listening to Pastor Troy or TI .\\nTrainSen\\t0\\tI JUST WANT BILL NYE TO COME TO EUGENE , OREGON SO I CAN HEAR THAT GENIUS OF A MAN SPEAK\\nTrainSen\\t0\\tI don't speak to my mom so that's not that funny\\nTrainSen\\t0\\tI'm so excited to speak at the National Church of God youngadults getaway .\\nTrainSen\\t0\\tyou are\\nTrainSen\\t0\\tIf you hear the way we speak to eachother you would never guess that We was together for so long the scruff face_with_medical_mask\\nTrainSen\\t0\\tAction speak audible than words you know akr so I rather gesture than ke bue now and not be able to convince you Bua pele\\nTrainSen\\t0\\tI don't fuck with prople smirking_face  so if I was to see you somewhere don't speak to me cause I don't know you OK_hand raised_fist face_with_tears_of_joy face_blowing_a_kiss\\nTrainSen\\t0\\tFirst day of senior year \\nTrainSen\\t0\\tSo they speak English rather than German  ur not going back to the old country then ;  x\\nTrainSen\\t0\\tGod didn't make it so we have to figure Him out He wants to speak with us  SHORT READ  Discerning God's Voice   \\\n",
      "0                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "1                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "2                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "3                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "4                                              False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "   Tweet_  Sex is so disgusting  procreation   ...  \\\n",
      "0                                       False  ...   \n",
      "1                                       False  ...   \n",
      "2                                       False  ...   \n",
      "3                                       False  ...   \n",
      "4                                       False  ...   \n",
      "\n",
      "   Tweet_you foregot the part  \\\n",
      "0                       False   \n",
      "1                       False   \n",
      "2                       False   \n",
      "3                       False   \n",
      "4                       False   \n",
      "\n",
      "   Tweet_you have no right to dislike the biggest movie of India Baahubali and like any other small budget movie winking_face_with_tongue  \\\n",
      "0                                              False                                                                                        \n",
      "1                                              False                                                                                        \n",
      "2                                              False                                                                                        \n",
      "3                                              False                                                                                        \n",
      "4                                              False                                                                                        \n",
      "\n",
      "   Tweet_you know what I love people who respond to stress by being horrible humans to everyone around them!  \\\n",
      "0                                              False                                                           \n",
      "1                                              False                                                           \n",
      "2                                              False                                                           \n",
      "3                                              False                                                           \n",
      "4                                              False                                                           \n",
      "\n",
      "   Tweet_you probably take kids candy as well  \\\n",
      "0                                       False   \n",
      "1                                       False   \n",
      "2                                       False   \n",
      "3                                       False   \n",
      "4                                       False   \n",
      "\n",
      "   Tweet_you too my lovely . I'm on holiday count down  I'm so flipping excited . Just trying to sort out streaky legs as we speak xx  \\\n",
      "0                                              False                                                                                    \n",
      "1                                              False                                                                                    \n",
      "2                                              False                                                                                    \n",
      "3                                              False                                                                                    \n",
      "4                                              False                                                                                    \n",
      "\n",
      "   Tweet_you'd think someone empowered to speak for equal rights   for gays , for blacks , for anyone   would do so . but you would not be panchito .  \\\n",
      "0                                              False                                                                                                    \n",
      "1                                              False                                                                                                    \n",
      "2                                              False                                                                                                    \n",
      "3                                              False                                                                                                    \n",
      "4                                              False                                                                                                    \n",
      "\n",
      "   Tweet_you're my bestfriend   Tweet_you're right , you're always right .  \\\n",
      "0                        False                                       False   \n",
      "1                        False                                       False   \n",
      "2                        False                                       False   \n",
      "3                        False                                       False   \n",
      "4                        False                                       False   \n",
      "\n",
      "   Tweet_you're such a good friend  \\\n",
      "0                            False   \n",
      "1                            False   \n",
      "2                            False   \n",
      "3                            False   \n",
      "4                            False   \n",
      "\n",
      "   Tweet_you've done enough damage to us all . You don't speak for us so you don't sign for us . Traitor .  \n",
      "0                                              False                                                        \n",
      "1                                              False                                                        \n",
      "2                                              False                                                        \n",
      "3                                              False                                                        \n",
      "4                                              False                                                        \n",
      "\n",
      "[5 rows x 2987 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('t_dataset.csv')\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(df['Tweet'], prefix='Tweet')\n",
    "\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "print(\"\\nOne-hot encoded DataFrame:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "df_encoded.to_csv('one_hot_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c056a54-954f-46c1-b394-d8e919611e4a",
   "metadata": {},
   "source": [
    "### Label Encoding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3fd4192-2890-48fa-9156-87347eb49133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tweet', 'Sarcasm', 'word_tokens', 'char_tokens', 'sentence_tokens',\n",
      "       'subword_tokens', 'lemmatized_tweet'],\n",
      "      dtype='object')\n",
      "Label Encoded DataFrame:\n",
      "                                               Tweet Sarcasm word_tokens  \\\n",
      "0  Fantastic service yet again from EE. 1st you u...     yes       [yes]   \n",
      "1  Not sure if that was or. I will take it! face_...     yes       [yes]   \n",
      "2         Barely 9 am and already shaking with rage.     yes       [yes]   \n",
      "3  I guess that proves it then. Black folks have ...     yes       [yes]   \n",
      "4                         Does this tweet need a tag     yes       [yes]   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  ['F', 'a', 'n', 't', 'a', 's', 't', 'i', 'c', ...   \n",
      "1  ['N', 'o', 't', ' ', 's', 'u', 'r', 'e', ' ', ...   \n",
      "2  ['B', 'a', 'r', 'e', 'l', 'y', ' ', '9', ' ', ...   \n",
      "3  ['I', ' ', 'g', 'u', 'e', 's', 's', ' ', 't', ...   \n",
      "4  ['D', 'o', 'e', 's', ' ', 't', 'h', 'i', 's', ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0  ['Fantastic service yet again from EE.', '1st ...   \n",
      "1  ['Not sure if that was or.', 'I will take it!'...   \n",
      "2     ['Barely 9 am and already shaking with rage.']   \n",
      "3  ['I guess that proves it then.', 'Black folks ...   \n",
      "4                     ['Does this tweet need a tag']   \n",
      "\n",
      "                                      subword_tokens  \\\n",
      "0  ['fantastic', 'service', 'yet', 'again', 'from...   \n",
      "1  ['not', 'sure', 'if', 'that', 'was', 'or', '.'...   \n",
      "2  ['barely', '9', 'am', 'and', 'already', 'shaki...   \n",
      "3  ['i', 'guess', 'that', 'proves', 'it', 'then',...   \n",
      "4  ['does', 'this', 't', '##wee', '##t', 'need', ...   \n",
      "\n",
      "                                    lemmatized_tweet  Category_Encoded  \n",
      "0  Fantastic service yet again from EE . 1st you ...                 1  \n",
      "1  Not sure if that wa or . I will take it ! face...                 1  \n",
      "2        Barely 9 am and already shaking with rage .                 1  \n",
      "3  I guess that prof it then . Black folk have no...                 1  \n",
      "4                         Does this tweet need a tag                 1  \n",
      "                                                  Tweet Sarcasm word_tokens  \\\n",
      "2976  Sometimes truth is glaring you in the face  bl...      no        [no]   \n",
      "2977    I just love not hanging out with my boyfriend .      no        [no]   \n",
      "2978  There is this 1 quince picture I have that I'm...      no        [no]   \n",
      "2979  I feel so ill at the moment that I cant speak ...      no        [no]   \n",
      "2980         I'm So Erked Nobody Better Not Speak To Me      no        [no]   \n",
      "\n",
      "                                            char_tokens  \\\n",
      "2976  ['S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', ...   \n",
      "2977  ['I', ' ', 'j', 'u', 's', 't', ' ', 'l', 'o', ...   \n",
      "2978  ['T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', ...   \n",
      "2979  ['I', ' ', 'f', 'e', 'e', 'l', ' ', 's', 'o', ...   \n",
      "2980  ['I', \"'\", 'm', ' ', 'S', 'o', ' ', 'E', 'r', ...   \n",
      "\n",
      "                                        sentence_tokens  \\\n",
      "2976  [\"Sometimes truth is glaring you in the face  ...   \n",
      "2977  ['I just love not hanging out with my boyfrien...   \n",
      "2978  [\"There is this 1 quince picture I have that I...   \n",
      "2979  [\"I feel so ill at the moment that I cant spea...   \n",
      "2980     [\"I'm So Erked Nobody Better Not Speak To Me\"]   \n",
      "\n",
      "                                         subword_tokens  \\\n",
      "2976  ['sometimes', 'truth', 'is', 'glaring', 'you',...   \n",
      "2977  ['i', 'just', 'love', 'not', 'hanging', 'out',...   \n",
      "2978  ['there', 'is', 'this', '1', 'qui', '##nce', '...   \n",
      "2979  ['i', 'feel', 'so', 'ill', 'at', 'the', 'momen...   \n",
      "2980  ['i', \"'\", 'm', 'so', 'er', '##ked', 'nobody',...   \n",
      "\n",
      "                                       lemmatized_tweet  Category_Encoded  \n",
      "2976  Sometimes truth is glaring you in the face bli...                 0  \n",
      "2977    I just love not hanging out with my boyfriend .                 0  \n",
      "2978  There is this 1 quince picture I have that I '...                 0  \n",
      "2979  I feel so ill at the moment that I cant speak ...                 0  \n",
      "2980        I 'm So Erked Nobody Better Not Speak To Me                 0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.split()  \n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def tokenize_characters(text):\n",
    "    return list(text) \n",
    "\n",
    "df = pd.read_csv('t_dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Sarcasm'].apply(tokenize_text)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Category_Encoded'] = le.fit_transform(df['Sarcasm'])\n",
    "\n",
    "print(\"Label Encoded DataFrame:\")\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "\n",
    "\n",
    "df.to_csv('label_encoded_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c219b74-64ca-43e3-8f2b-4b293a68b111",
   "metadata": {},
   "source": [
    "### TF-IDF Encoding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b1b16e1-d8ed-4d9a-9c32-04bca109b105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Encoded DataFrame:\n",
      "                                               Tweet Sarcasm word_tokens  \\\n",
      "0  Fantastic service yet again from EE. 1st you u...     yes       [yes]   \n",
      "1  Not sure if that was or. I will take it! face_...     yes       [yes]   \n",
      "2         Barely 9 am and already shaking with rage.     yes       [yes]   \n",
      "3  I guess that proves it then. Black folks have ...     yes       [yes]   \n",
      "4                         Does this tweet need a tag     yes       [yes]   \n",
      "\n",
      "                                         char_tokens  \\\n",
      "0  ['F', 'a', 'n', 't', 'a', 's', 't', 'i', 'c', ...   \n",
      "1  ['N', 'o', 't', ' ', 's', 'u', 'r', 'e', ' ', ...   \n",
      "2  ['B', 'a', 'r', 'e', 'l', 'y', ' ', '9', ' ', ...   \n",
      "3  ['I', ' ', 'g', 'u', 'e', 's', 's', ' ', 't', ...   \n",
      "4  ['D', 'o', 'e', 's', ' ', 't', 'h', 'i', 's', ...   \n",
      "\n",
      "                                     sentence_tokens  \\\n",
      "0  ['Fantastic service yet again from EE.', '1st ...   \n",
      "1  ['Not sure if that was or.', 'I will take it!'...   \n",
      "2     ['Barely 9 am and already shaking with rage.']   \n",
      "3  ['I guess that proves it then.', 'Black folks ...   \n",
      "4                     ['Does this tweet need a tag']   \n",
      "\n",
      "                                      subword_tokens  \\\n",
      "0  ['fantastic', 'service', 'yet', 'again', 'from...   \n",
      "1  ['not', 'sure', 'if', 'that', 'was', 'or', '.'...   \n",
      "2  ['barely', '9', 'am', 'and', 'already', 'shaki...   \n",
      "3  ['i', 'guess', 'that', 'proves', 'it', 'then',...   \n",
      "4  ['does', 'this', 't', '##wee', '##t', 'need', ...   \n",
      "\n",
      "                                    lemmatized_tweet   10   12   15  ...  \\\n",
      "0  Fantastic service yet again from EE . 1st you ...  0.0  0.0  0.0  ...   \n",
      "1  Not sure if that wa or . I will take it ! face...  0.0  0.0  0.0  ...   \n",
      "2        Barely 9 am and already shaking with rage .  0.0  0.0  0.0  ...   \n",
      "3  I guess that prof it then . Black folk have no...  0.0  0.0  0.0  ...   \n",
      "4                         Does this tweet need a tag  0.0  0.0  0.0  ...   \n",
      "\n",
      "   years  yes  yesterday       yet   yo       you  young  your  youre  \\\n",
      "0    0.0  0.0        0.0  0.283748  0.0  0.248577    0.0   0.0    0.0   \n",
      "1    0.0  0.0        0.0  0.000000  0.0  0.000000    0.0   0.0    0.0   \n",
      "2    0.0  0.0        0.0  0.000000  0.0  0.000000    0.0   0.0    0.0   \n",
      "3    0.0  0.0        0.0  0.000000  0.0  0.000000    0.0   0.0    0.0   \n",
      "4    0.0  0.0        0.0  0.000000  0.0  0.000000    0.0   0.0    0.0   \n",
      "\n",
      "   yourself  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "\n",
      "[5 rows x 1007 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def tokenize_characters(text):\n",
    "    return list(text)  \n",
    "\n",
    "df = pd.read_csv('t_dataset.csv')\n",
    "\n",
    "df['word_tokens'] = df['Sarcasm'].apply(tokenize_text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  \n",
    "tfidf_matrix = vectorizer.fit_transform(df['Tweet'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_encoded = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "print(\"TF-IDF Encoded DataFrame:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "df_encoded.to_csv('tfidf_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c52598-e8ae-4ba1-b34d-547b88115e2b",
   "metadata": {},
   "source": [
    "### Term Frequency Encoder;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3319df58-b82e-401d-bf6e-4fb73b0f3d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sarcasm  000  045  050  06  10  100  100m  100s  1010  ...  zayn  zealander  \\\n",
      "0     yes    0    0    0   0   0    0     0     0     0  ...     0          0   \n",
      "1     yes    0    0    0   0   0    0     0     0     0  ...     0          0   \n",
      "2     yes    0    0    0   0   0    0     0     0     0  ...     0          0   \n",
      "3     yes    0    0    0   0   0    0     0     0     0  ...     0          0   \n",
      "4     yes    0    0    0   0   0    0     0     0     0  ...     0          0   \n",
      "\n",
      "   zen  zero  zion  zombie  zoom  zorra  zt  zzz  \n",
      "0    0     0     0       0     0      0   0    0  \n",
      "1    0     0     0       0     0      0   0    0  \n",
      "2    0     0     0       0     0      0   0    0  \n",
      "3    0     0     0       0     0      0   0    0  \n",
      "4    0     0     0       0     0      0   0    0  \n",
      "\n",
      "[5 rows x 6069 columns]\n",
      "     Sarcasm  000  045  050  06  10  100  100m  100s  1010  ...  zayn  \\\n",
      "2976      no    0    0    0   0   0    0     0     0     0  ...     0   \n",
      "2977      no    0    0    0   0   0    0     0     0     0  ...     0   \n",
      "2978      no    0    0    0   0   0    0     0     0     0  ...     0   \n",
      "2979      no    0    0    0   0   0    0     0     0     0  ...     0   \n",
      "2980      no    0    0    0   0   0    0     0     0     0  ...     0   \n",
      "\n",
      "      zealander  zen  zero  zion  zombie  zoom  zorra  zt  zzz  \n",
      "2976          0    0     0     0       0     0      0   0    0  \n",
      "2977          0    0     0     0       0     0      0   0    0  \n",
      "2978          0    0     0     0       0     0      0   0    0  \n",
      "2979          0    0     0     0       0     0      0   0    0  \n",
      "2980          0    0     0     0       0     0      0   0    0  \n",
      "\n",
      "[5 rows x 6069 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv('t_dataset.csv')\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "tf_matrix = count_vectorizer.fit_transform(df['Tweet'])\n",
    "\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "tf_encoded_df = pd.concat([df['Sarcasm'], tf_df], axis=1)\n",
    "\n",
    "print(tf_encoded_df.head())\n",
    "print(tf_encoded_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea712d-efc5-4e6e-a81f-9dae840794b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
