{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73b7139-ffc4-4efd-b894-fb607903bce6",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274da1d6-b2e3-4d56-8312-5ceca760399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7693f94-a793-4863-914f-ddc89a8cffc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic service yet again from EE. 1st you u...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure if that was or. I will take it! face_...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barely 9 am and already shaking with rage.</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I guess that proves it then. Black folks have ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does this tweet need a tag</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Sarcasm\n",
       "0  Fantastic service yet again from EE. 1st you u...     yes\n",
       "1  Not sure if that was or. I will take it! face_...     yes\n",
       "2         Barely 9 am and already shaking with rage.     yes\n",
       "3  I guess that proves it then. Black folks have ...     yes\n",
       "4                         Does this tweet need a tag     yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('merged_file.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea90b88-2552-4d22-ae29-8dd67f569fe7",
   "metadata": {},
   "source": [
    "# Different ways of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ec144-f5d4-432b-9050-e11cb7dcde1b",
   "metadata": {},
   "source": [
    "## 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcfed57-fb5f-4828-9315-953612fc5186",
   "metadata": {},
   "source": [
    "## 2. Character Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e899916-8fc7-41e7-90d2-2eaa74f1ca80",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfcff3-5d4f-4cdd-9480-1589ee9abac8",
   "metadata": {},
   "source": [
    "## 4. Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b157b4-ff54-461d-b13c-d6e3f6990cda",
   "metadata": {},
   "source": [
    "# Technique Number 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39763c4-0d40-4d4f-895b-ae7f1fb7214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                               tokens  \n",
      "0   [Fantastic, service, yet, again, from, EE, ., ...  \n",
      "1   [Not, sure, if, that, was, or, ., I, will, tak...  \n",
      "2   [Barely, 9, am, and, already, shaking, with, r...  \n",
      "3   [I, guess, that, proves, it, then, ., Black, f...  \n",
      "4                   [Does, this, tweet, need, a, tag]  \n",
      "5   [both, ., Wont, be, using, you, again, ., Made...  \n",
      "6      [Fuuuuuuuuck, this, shit, screams, thug, !, !]  \n",
      "7   [66, UEs, from, Staniel, and, he, wins, in, st...  \n",
      "8       [totally, surprising, to, every, husker, fan]  \n",
      "9              [Haha, got, to, love, the, enthusiasm]  \n",
      "10  [a, southern, pride, advocate, but, of, course...  \n",
      "11  [So, nice, not, hearing, any, rumors, that, th...  \n",
      "12  [We, should, divide, illegals, into, two, line...  \n",
      "13  [does, not, even, look, like, that, great, of,...  \n",
      "14  [Mondays, are, always, dreadful, ., Why, are, ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Initialize the TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(Tweet):\n",
    "    return tokenizer.tokenize(Tweet)\n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "df['tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "print(df.head(15))\n",
    "df.to_csv('t_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932c83f-0d3b-47d4-ae7f-083918ee4003",
   "metadata": {},
   "source": [
    "# Technique Number 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e7e200-c29c-484d-856c-41bee07e6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [Fantastic, service, yet, again, from, EE, ., ...   \n",
      "1   [Not, sure, if, that, was, or, ., I, will, tak...   \n",
      "2   [Barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [I, guess, that, proves, it, then, ., Black, f...   \n",
      "4                   [Does, this, tweet, need, a, tag]   \n",
      "5   [both, ., Wont, be, using, you, again, ., Made...   \n",
      "6      [Fuuuuuuuuck, this, shit, screams, thug, !, !]   \n",
      "7   [66, UEs, from, Staniel, and, he, wins, in, st...   \n",
      "8       [totally, surprising, to, every, husker, fan]   \n",
      "9              [Haha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [So, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [We, should, divide, illegals, into, two, line...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [Mondays, are, always, dreadful, ., Why, are, ...   \n",
      "\n",
      "                                          char_tokens  \n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...  \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...  \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...  \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...  \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...  \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...  \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...  \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...  \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...  \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...  \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...  \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...  \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...  \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...  \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def tokenize_characters(text):\n",
    "    return list(text)  \n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('t_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b441e5-8eef-45ae-bfbd-83cd88ef5b66",
   "metadata": {},
   "source": [
    "# Technique Number 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3af15755-308a-4139-93d5-39ece4c02f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jiyagoyal/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [Fantastic, service, yet, again, from, EE, ., ...   \n",
      "1   [Not, sure, if, that, was, or, ., I, will, tak...   \n",
      "2   [Barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [I, guess, that, proves, it, then, ., Black, f...   \n",
      "4                   [Does, this, tweet, need, a, tag]   \n",
      "5   [both, ., Wont, be, using, you, again, ., Made...   \n",
      "6      [Fuuuuuuuuck, this, shit, screams, thug, !, !]   \n",
      "7   [66, UEs, from, Staniel, and, he, wins, in, st...   \n",
      "8       [totally, surprising, to, every, husker, fan]   \n",
      "9              [Haha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [So, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [We, should, divide, illegals, into, two, line...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [Mondays, are, always, dreadful, ., Why, are, ...   \n",
      "\n",
      "                                          char_tokens  \\\n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...   \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...   \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...   \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...   \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...   \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...   \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...   \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...   \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...   \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...   \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...   \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...   \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...   \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...   \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...   \n",
      "\n",
      "                                      sentence_tokens  \n",
      "0   [Fantastic service yet again from EE., 1st you...  \n",
      "1   [Not sure if that was or., I will take it!, fa...  \n",
      "2        [Barely 9 am and already shaking with rage.]  \n",
      "3   [I guess that proves it then., Black folks hav...  \n",
      "4                        [Does this tweet need a tag]  \n",
      "5   [both., Wont be using you again., Made a forma...  \n",
      "6            [Fuuuuuuuuck this shit screams thug!, !]  \n",
      "7   [66 UEs from Staniel and he wins in straights....  \n",
      "8            [totally surprising to every husker fan]  \n",
      "9                   [Haha got to love the enthusiasm]  \n",
      "10  [a southern pride advocate but of course it ha...  \n",
      "11  [So nice not hearing any rumors that the  are ...  \n",
      "12  [We should divide illegals into two lines one ...  \n",
      "13    [does not even look like that great of a view.]  \n",
      "14  [Mondays are always dreadful., Why are Fridays...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "    \n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweet'].apply(tokenize_sentences)\n",
    "print(df.head(15))\n",
    "df.to_csv('t_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0deeb3c-6c75-46e7-8d32-4bde3ccbcb24",
   "metadata": {},
   "source": [
    "# Technique Number 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c6a08d0-42c6-4c49-b16b-bec9b170801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.3/410.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a187b4db-408a-4d52-8583-d34b71ac7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [fantastic, service, yet, again, from, ee, ., ...   \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...   \n",
      "2   [barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [i, guess, that, proves, it, then, ., black, f...   \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]   \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...   \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...   \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...   \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...   \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...   \n",
      "\n",
      "                                          char_tokens  \\\n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...   \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...   \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...   \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...   \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...   \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...   \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...   \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...   \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...   \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...   \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...   \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...   \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...   \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...   \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...   \n",
      "\n",
      "                                      sentence_tokens  \\\n",
      "0   [Fantastic service yet again from EE., 1st you...   \n",
      "1   [Not sure if that was or., I will take it!, fa...   \n",
      "2        [Barely 9 am and already shaking with rage.]   \n",
      "3   [I guess that proves it then., Black folks hav...   \n",
      "4                        [Does this tweet need a tag]   \n",
      "5   [both., Wont be using you again., Made a forma...   \n",
      "6            [Fuuuuuuuuck this shit screams thug!, !]   \n",
      "7   [66 UEs from Staniel and he wins in straights....   \n",
      "8            [totally surprising to every husker fan]   \n",
      "9                   [Haha got to love the enthusiasm]   \n",
      "10  [a southern pride advocate but of course it ha...   \n",
      "11  [So nice not hearing any rumors that the  are ...   \n",
      "12  [We should divide illegals into two lines one ...   \n",
      "13    [does not even look like that great of a view.]   \n",
      "14  [Mondays are always dreadful., Why are Fridays...   \n",
      "\n",
      "                                       subword_tokens  \n",
      "0   [fantastic, service, yet, again, from, ee, ., ...  \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...  \n",
      "2   [barely, 9, am, and, already, shaking, with, r...  \n",
      "3   [i, guess, that, proves, it, then, ., black, f...  \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]  \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...  \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...  \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...  \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...  \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]  \n",
      "10  [a, southern, pride, advocate, but, of, course...  \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...  \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...  \n",
      "13  [does, not, even, look, like, that, great, of,...  \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_subwords(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweet'].apply(tokenize_sentences)\n",
    "\n",
    "df['subword_tokens'] = df['Tweet'].apply(tokenize_subwords)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('t_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c9705-209d-441c-ac8c-40ea2a9cc0c1",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b1cba9f-6161-4a63-b894-a60b383cd0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jiyagoyal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jiyagoyal/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweet Sarcasm  \\\n",
      "0   Fantastic service yet again from EE. 1st you u...     yes   \n",
      "1   Not sure if that was or. I will take it! face_...     yes   \n",
      "2          Barely 9 am and already shaking with rage.     yes   \n",
      "3   I guess that proves it then. Black folks have ...     yes   \n",
      "4                          Does this tweet need a tag     yes   \n",
      "5   both. Wont be using you again. Made a formal c...     yes   \n",
      "6                Fuuuuuuuuck this shit screams thug!!     yes   \n",
      "7   66 UEs from Staniel and he wins in straights. ...     yes   \n",
      "8              totally surprising to every husker fan     yes   \n",
      "9                     Haha got to love the enthusiasm     yes   \n",
      "10  a southern pride advocate but of course it has...     yes   \n",
      "11  So nice not hearing any rumors that the  are n...     yes   \n",
      "12  We should divide illegals into two lines one f...     yes   \n",
      "13      does not even look like that great of a view.     yes   \n",
      "14  Mondays are always dreadful. Why are Fridays s...     yes   \n",
      "\n",
      "                                          word_tokens  \\\n",
      "0   [fantastic, service, yet, again, from, ee, ., ...   \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...   \n",
      "2   [barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [i, guess, that, proves, it, then, ., black, f...   \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]   \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...   \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...   \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...   \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...   \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...   \n",
      "\n",
      "                                          char_tokens  \\\n",
      "0   [F, a, n, t, a, s, t, i, c,  , s, e, r, v, i, ...   \n",
      "1   [N, o, t,  , s, u, r, e,  , i, f,  , t, h, a, ...   \n",
      "2   [B, a, r, e, l, y,  , 9,  , a, m,  , a, n, d, ...   \n",
      "3   [I,  , g, u, e, s, s,  , t, h, a, t,  , p, r, ...   \n",
      "4   [D, o, e, s,  , t, h, i, s,  , t, w, e, e, t, ...   \n",
      "5   [b, o, t, h, .,  , W, o, n, t,  , b, e,  , u, ...   \n",
      "6   [F, u, u, u, u, u, u, u, u, c, k,  , t, h, i, ...   \n",
      "7   [6, 6,  , U, E, s,  , f, r, o, m,  , S, t, a, ...   \n",
      "8   [t, o, t, a, l, l, y,  , s, u, r, p, r, i, s, ...   \n",
      "9   [H, a, h, a,  , g, o, t,  , t, o,  , l, o, v, ...   \n",
      "10  [a,  , s, o, u, t, h, e, r, n,  , p, r, i, d, ...   \n",
      "11  [S, o,  , n, i, c, e,  , n, o, t,  , h, e, a, ...   \n",
      "12  [W, e,  , s, h, o, u, l, d,  , d, i, v, i, d, ...   \n",
      "13  [d, o, e, s,  , n, o, t,  , e, v, e, n,  , l, ...   \n",
      "14  [M, o, n, d, a, y, s,  , a, r, e,  , a, l, w, ...   \n",
      "\n",
      "                                      sentence_tokens  \\\n",
      "0   [Fantastic service yet again from EE., 1st you...   \n",
      "1   [Not sure if that was or., I will take it!, fa...   \n",
      "2        [Barely 9 am and already shaking with rage.]   \n",
      "3   [I guess that proves it then., Black folks hav...   \n",
      "4                        [Does this tweet need a tag]   \n",
      "5   [both., Wont be using you again., Made a forma...   \n",
      "6            [Fuuuuuuuuck this shit screams thug!, !]   \n",
      "7   [66 UEs from Staniel and he wins in straights....   \n",
      "8            [totally surprising to every husker fan]   \n",
      "9                   [Haha got to love the enthusiasm]   \n",
      "10  [a southern pride advocate but of course it ha...   \n",
      "11  [So nice not hearing any rumors that the  are ...   \n",
      "12  [We should divide illegals into two lines one ...   \n",
      "13    [does not even look like that great of a view.]   \n",
      "14  [Mondays are always dreadful., Why are Fridays...   \n",
      "\n",
      "                                       subword_tokens  \\\n",
      "0   [fantastic, service, yet, again, from, ee, ., ...   \n",
      "1   [not, sure, if, that, was, or, ., i, will, tak...   \n",
      "2   [barely, 9, am, and, already, shaking, with, r...   \n",
      "3   [i, guess, that, proves, it, then, ., black, f...   \n",
      "4           [does, this, t, ##wee, ##t, need, a, tag]   \n",
      "5   [both, ., won, ##t, be, using, you, again, ., ...   \n",
      "6   [fu, ##u, ##u, ##u, ##u, ##u, ##u, ##uck, this...   \n",
      "7   [66, u, ##es, from, stan, ##iel, and, he, wins...   \n",
      "8   [totally, surprising, to, every, hu, ##ske, ##...   \n",
      "9          [ha, ##ha, got, to, love, the, enthusiasm]   \n",
      "10  [a, southern, pride, advocate, but, of, course...   \n",
      "11  [so, nice, not, hearing, any, rumors, that, th...   \n",
      "12  [we, should, divide, illegal, ##s, into, two, ...   \n",
      "13  [does, not, even, look, like, that, great, of,...   \n",
      "14  [mondays, are, always, dreadful, ., why, are, ...   \n",
      "\n",
      "                                     lemmatized_tweet  \n",
      "0   Fantastic service yet again from EE . 1st you ...  \n",
      "1   Not sure if that wa or . I will take it ! face...  \n",
      "2         Barely 9 am and already shaking with rage .  \n",
      "3   I guess that prof it then . Black folk have no...  \n",
      "4                          Does this tweet need a tag  \n",
      "5   both . Wont be using you again . Made a formal...  \n",
      "6               Fuuuuuuuuck this shit scream thug ! !  \n",
      "7   66 UEs from Staniel and he win in straight . B...  \n",
      "8              totally surprising to every husker fan  \n",
      "9                     Haha got to love the enthusiasm  \n",
      "10  a southern pride advocate but of course it ha ...  \n",
      "11  So nice not hearing any rumor that the are not...  \n",
      "12  We should divide illegals into two line one fo...  \n",
      "13      doe not even look like that great of a view .  \n",
      "14  Mondays are always dreadful . Why are Fridays ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "df['word_tokens'] = df['Tweet'].apply(tokenize_text)\n",
    "\n",
    "df['char_tokens'] = df['Tweet'].apply(tokenize_characters)\n",
    "\n",
    "df['sentence_tokens'] = df['Tweet'].apply(tokenize_sentences)\n",
    "\n",
    "df['subword_tokens'] = df['Tweet'].apply(tokenize_subwords)\n",
    "\n",
    "df['lemmatized_tweet'] = df['Tweet'].apply(lemmatize_text)\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "df.to_csv('t_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea712d-efc5-4e6e-a81f-9dae840794b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
